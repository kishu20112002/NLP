{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26e75996-538d-4739-b4bf-b2027db28359",
   "metadata": {},
   "source": [
    "# => Spacy library\n",
    "\n",
    "- Spacy flow\n",
    "  - string\n",
    "  - Document\n",
    "  - Tokens\n",
    "    - Atributes\n",
    "      1. t.text\n",
    "      2. t.is_alpha\n",
    "      3. t.is_stop\n",
    "      4. t.is_punct\n",
    "      5. t.lemma_\n",
    "      6. t.pos_\n",
    "      7. entity.label_\n",
    "\n",
    "## ==> NLP using Spacy Library\n",
    "### Python => To create a program(FP + OOPS)\n",
    "### Numpy  => To create array\n",
    "### Pandas => To do Data Analysis\n",
    "### Matplotlib => To do Viz\n",
    "### seaborn => To do Viz\n",
    "### sklearn  => To create models\n",
    "### nltk => To create analytics (Basic one)\n",
    "### spacy => To do text Analytics\n",
    "\n",
    "\n",
    "## 1 Introduction\n",
    "- There are mainly 3 types of data we have\n",
    "  - Strutured data\n",
    "  - Semi strutured data\n",
    "  - Unstrutured data\n",
    "- Today more than 80% of the availables as Unstructured Data\n",
    "- Unstructured data is a data which cannot be represented in tabular format\n",
    "  - Text\n",
    "  - Video\n",
    "  - Image\n",
    "- We can use Natual Language processing to process and interpret the unstructured data\n",
    "- NLP application\n",
    "  - Sentiment analysis\n",
    "  - Speech recognition\n",
    "  - Text Classification\n",
    "  - Machine Translation\n",
    "  - Semantic Search\n",
    "  - News/article Summarization\n",
    "  - Answering Questions\n",
    "    \n",
    "- Live example\n",
    "  - Google Assistant\n",
    "  - Amazon Echo\n",
    "  - ChatBot\n",
    "  - Language Translations\n",
    "  - Article summarization and so on\n",
    "\n",
    "\n",
    "## 2 Install and load main python libraries for NLP\n",
    "- We do have different python libraries to work with NLP\n",
    "  - Nltk\n",
    "  - Spacy\n",
    "  - Genism\n",
    "  - transformers\n",
    "\n",
    "### 2.1 Installation\n",
    "- Open a command prompt and run below commands\n",
    "  - pip install nltk\n",
    "  - pip install spacy\n",
    "  - pip install gensim\n",
    "  - pip install transformers\n",
    "\n",
    "## Note : Run below command \n",
    "- python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f44ff402-4bb7-493d-9439-d80020f08077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/miniconda3/lib/python3.13/site-packages (3.9.2)\n",
      "Requirement already satisfied: click in /opt/miniconda3/lib/python3.13/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /opt/miniconda3/lib/python3.13/site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/miniconda3/lib/python3.13/site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/lib/python3.13/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c924efd-02dc-46be-923a-f1c4d314e413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.11-cp310-cp310-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.15-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.3 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.13-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.7 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.12-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.10-cp310-cp310-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.4.2 (from spacy)\n",
      "  Downloading weasel-0.4.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer-slim<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer_slim-0.21.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from spacy) (2.32.5)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from spacy) (25.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.41.5-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.5 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wrapt in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from jinja2->spacy) (3.0.3)\n",
      "Downloading spacy-3.8.11-cp310-cp310-macosx_11_0_arm64.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.13-cp310-cp310-macosx_11_0_arm64.whl (43 kB)\n",
      "Downloading murmurhash-1.0.15-cp310-cp310-macosx_11_0_arm64.whl (27 kB)\n",
      "Downloading preshed-3.0.12-cp310-cp310-macosx_11_0_arm64.whl (124 kB)\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp310-cp310-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.2-cp310-cp310-macosx_11_0_arm64.whl (653 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m653.4/653.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "Downloading thinc-8.3.10-cp310-cp310-macosx_11_0_arm64.whl (772 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.3/772.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "Downloading blis-1.3.3-cp310-cp310-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading typer_slim-0.21.1-py3-none-any.whl (47 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.3-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
      "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: wasabi, typing-inspection, typer-slim, spacy-loggers, spacy-legacy, smart-open, pydantic-core, murmurhash, cymem, cloudpathlib, catalogue, blis, annotated-types, srsly, pydantic, preshed, confection, weasel, thinc, spacy\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/20\u001b[0m [spacy]m19/20\u001b[0m [spacy]]c]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed annotated-types-0.7.0 blis-1.3.3 catalogue-2.0.10 cloudpathlib-0.23.0 confection-0.1.5 cymem-2.0.13 murmurhash-1.0.15 preshed-3.0.12 pydantic-2.12.5 pydantic-core-2.41.5 smart-open-7.5.0 spacy-3.8.11 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.2 thinc-8.3.10 typer-slim-0.21.1 typing-inspection-0.4.2 wasabi-1.1.3 weasel-0.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41705a5e-d4b1-47a9-a1cc-319e91032bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from gensim) (1.15.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
      "Downloading gensim-4.4.0-cp310-cp310-macosx_11_0_arm64.whl (24.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.4/24.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c0f2307-ca0f-442a-b228-3f45d5e1e7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.57.6-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.20.3-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading fsspec-2026.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from requests->transformers) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from requests->transformers) (2026.1.4)\n",
      "Downloading transformers-4.57.6-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.2-cp39-abi3-macosx_11_0_arm64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2026.1.0-py3-none-any.whl (201 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl (447 kB)\n",
      "Downloading filelock-3.20.3-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: safetensors, hf-xet, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed filelock-3.20.3 fsspec-2026.1.0 hf-xet-1.2.0 huggingface-hub-0.36.0 safetensors-0.7.0 tokenizers-0.22.2 transformers-4.57.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "320c2f94-6193-420b-899c-3910407ad101",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (581980377.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    python -m spacy download en_core_web_sm\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a8f94b0-0519-477c-b77b-1306438e453b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/bin/python: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2e85970-0f66-4790-a350-cf6f7f610ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (3.8.11)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from spacy) (0.21.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from spacy) (2.12.5)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from jinja2->spacy) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement en-core-web-sm (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for en-core-web-sm\u001b[0m\u001b[31m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy\n",
    "%pip install en-core-web-sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7aebaf6-e0d4-4a96-a9bd-c3f70d6b5947",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel loaded successfully ✅\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages/spacy/__init__.py:52\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     29\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     36\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages/spacy/util.py:531\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 531\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"Model loaded successfully ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c06365b4-b741-48ba-b6c2-50c51ea8477f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/bin/python: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6289506-883a-4d8c-bd6e-02d371005680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e521aa-ec1c-456a-9a97-cc785e107376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"✅ Model loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f4831e-4fdb-4f7f-9419-ff3e11b625e6",
   "metadata": {},
   "source": [
    "## 3 Text Pre-processing \n",
    "- The raw text data also called as text corpus it has a lot of noise\n",
    "- Raw text having punctuation special symblos stop words & etc\n",
    "- From the raw data we need to process the text by using nip techniques\n",
    "- A text can be converted into nip object of spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc276f74-b77b-469a-9691-7bcc0b941ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP is a very powerful toot\n",
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "# spacy.example\n",
    "\n",
    "import spacy \n",
    "\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "raw_text='NLP is a very powerful toot'\n",
    "\n",
    "text_doc=nlp(raw_text)\n",
    "\n",
    "print(text_doc)\n",
    "print(type(text_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa2de6f-7217-4192-b154-862d14b5ada2",
   "metadata": {},
   "source": [
    "## 4 What is Tokenization\n",
    "- The process of extracting tokens from a text file/document is referred as tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5d445be-c3f9-4c20-82ab-a2c1d55d6dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP is very powerful tool\n",
      "\n",
      "NLP\n",
      "is\n",
      "very\n",
      "powerful\n",
      "tool\n"
     ]
    }
   ],
   "source": [
    "# Tokenization using spacy\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "raw_text='NLP is very powerful tool'\n",
    "\n",
    "text_doc=nlp(raw_text)\n",
    "\n",
    "print(text_doc)\n",
    "print()\n",
    "\n",
    "for words in text_doc:\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c02cd18a-1335-4656-8133-d5c47a2ebde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP is a very powerful tool\n",
      "<class 'str'>\n",
      "\n",
      "Done\n",
      "\n",
      "NLP is a very powerful tool\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "NLP\n",
      "is\n",
      "a\n",
      "very\n",
      "powerful\n",
      "tool\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "a=spacy.load('en_core_web_sm')\n",
    "b='NLP is a very powerful tool'\n",
    "\n",
    "c=a(b)\n",
    "\n",
    "print(b)\n",
    "print(type(b))\n",
    "print()\n",
    "\n",
    "\n",
    "print('Done')\n",
    "print()\n",
    "\n",
    "print(c)\n",
    "print(type(c))\n",
    "\n",
    "for i in c:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9218d078-bd37-409c-b71a-0f461f43fdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP is a very powerful tool\n",
      "<class 'str'>\n",
      "\n",
      "Done\n",
      "\n",
      "NLP is a very powerful tool\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "NLP\n",
      "is\n",
      "a\n",
      "very\n",
      "powerful\n",
      "tool\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import spacy\n",
    "\n",
    "a=spacy.load('en_core_web_sm')\n",
    "b='NLP is a very powerful tool'\n",
    "\n",
    "c=a(b)\n",
    "\n",
    "print(b)\n",
    "print(type(b))\n",
    "print()\n",
    "\n",
    "\n",
    "print('Done')\n",
    "print()\n",
    "\n",
    "print(c)\n",
    "print(type(c))\n",
    "\n",
    "for i in c:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbba6708-22e1-46a6-bb87-a4fcca5ac866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP is a very powerful tool\n",
      "<class 'str'>\n",
      "\n",
      "Done\n",
      "\n",
      "NLP is a very powerful tool\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "NLP <class 'str'>\n",
      "is <class 'str'>\n",
      "a <class 'str'>\n",
      "very <class 'str'>\n",
      "powerful <class 'str'>\n",
      "tool <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import spacy\n",
    "\n",
    "a=spacy.load('en_core_web_sm')\n",
    "b='NLP is a very powerful tool'\n",
    "\n",
    "c=a(b)\n",
    "\n",
    "print(b)\n",
    "print(type(b))\n",
    "print()\n",
    "\n",
    "\n",
    "print('Done')\n",
    "print()\n",
    "\n",
    "print(c)\n",
    "print(type(c))\n",
    "\n",
    "for i in c:\n",
    "    print(i.text, type(i.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b95706c-6f34-466e-acac-59b9c5a14d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My salary is $1@0 dollars\n",
      "\n",
      "My         True\n",
      "salary     True\n",
      "is         True\n",
      "$          False\n",
      "1@0        False\n",
      "dollars    True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Checking every token either it is alphabet or not by using spacy\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "mixed_text='My salary is $1@0 dollars'\n",
    "mixed_text_doc=nlp(mixed_text)\n",
    "\n",
    "print(mixed_text_doc)\n",
    "print()\n",
    "for word in mixed_text_doc:\n",
    "    print(word.text.ljust(10),word.is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6201bd5a-a8fd-46ba-9b05-bfd3aa4cf8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My salary is $1@0 dollars\n",
      "\n",
      "My   True\n",
      "salary   True\n",
      "is   True\n",
      "$   False\n",
      "1@0   False\n",
      "dollars   True\n"
     ]
    }
   ],
   "source": [
    "## Checking every token either it is alphabet or not by using spacy\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "mixed_text='My salary is $1@0 dollars'\n",
    "mixed_text_doc=nlp(mixed_text)\n",
    "\n",
    "print(mixed_text_doc)\n",
    "print()\n",
    "for word in mixed_text_doc:\n",
    "    print(word, \" \",word.is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecd1ea6c-1d03-42ef-a040-2b670df02054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My salary is $1@0 dollars\n",
      "\n",
      "My\n",
      "salary\n",
      "is\n",
      "dollars\n"
     ]
    }
   ],
   "source": [
    "## Checking every token either it is alphabet or not by using spacy printing only words\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "mixed_text='My salary is $1@0 dollars'\n",
    "mixed_text_doc=nlp(mixed_text)\n",
    "\n",
    "print(mixed_text_doc)\n",
    "print()\n",
    "for word in mixed_text_doc:\n",
    "    if word.is_alpha:\n",
    "        print(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7257b654-5000-44ca-8275-192fb3705692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My salary is $1@0 dollars\n",
      "\n",
      "$\n",
      "1@0\n"
     ]
    }
   ],
   "source": [
    "## Checking every token either it is alphabet or not by using spacy printing only special symbole words\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "mixed_text='My salary is $1@0 dollars'\n",
    "mixed_text_doc=nlp(mixed_text)\n",
    "\n",
    "print(mixed_text_doc)\n",
    "print()\n",
    "for word in mixed_text_doc:\n",
    "    if not word.is_alpha:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f134ac6-bdbc-47cf-9bc0-79c506d3fa63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text \t\t Alpha Stop Punct\n",
      "\n",
      "My              : True True False\n",
      "slary           : True False False\n",
      "is              : True True False\n",
      "$               : False False False\n",
      "1000            : False False False\n",
      "dollars         : True False False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Checking every token alpha stop word and punctuation\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "text='My slary is $1000 dollars'\n",
    "text_doc=nlp(text)\n",
    "\n",
    "print('Text',\n",
    "      '\\t\\t',\n",
    "      'Alpha',\n",
    "      'Stop',\n",
    "      'Punct')\n",
    "\n",
    "print()\n",
    "for word in text_doc:\n",
    "    print(word.text.ljust(15),\n",
    "          ':',\n",
    "          word.is_alpha,\n",
    "          word.is_stop,\n",
    "          word.is_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb3c16c0-4e50-46f4-83f7-3890c2b532a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking tokens ans count of tokens by using spacy\n",
    "\n",
    "import spacy \n",
    "\n",
    "nlb=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b22bdedb-01bc-4d80-9464-c7bf04cab5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "value=[10,20,30,40]\n",
    "\n",
    "print(len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a79df23-6b9e-4d57-9a74-523ba8ad61a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "values=[10,20,30,40]\n",
    "\n",
    "counter=0\n",
    "\n",
    "for value in values:\n",
    "    counter=counter+1\n",
    "\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec718cc4-4f0f-4c20-b55e-ba4c70660e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f20efd73-0003-4c70-b83e-3239d176c69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon\n",
      "Alexa\n",
      ",\n",
      "also\n",
      "known\n",
      "simply\n",
      "as\n",
      "Alexa\n",
      ",\n",
      "is\n",
      "a\n",
      "virtual\n",
      "assistant\n",
      "AI\n",
      "technology\n",
      "\n",
      "\n",
      "developed\n",
      "by\n",
      "Amazon\n",
      ",\n",
      "first\n",
      "used\n",
      "in\n",
      "the\n",
      "Amazon\n",
      "Echo\n",
      "smart\n",
      "speakers\n",
      "developed\n",
      "by\n",
      "Amazon\n",
      "Lab126\n",
      ".\n",
      "\n",
      "\n",
      "It\n",
      "is\n",
      "capable\n",
      "of\n",
      "voice\n",
      "interaction\n",
      ",\n",
      "music\n",
      "playback\n",
      ",\n",
      "making\n",
      "to\n",
      "-\n",
      "do\n",
      "lists\n",
      ",\n",
      "setting\n",
      "alarms\n",
      ",\n",
      "\n",
      "\n",
      "streaming\n",
      "podcasts\n",
      ",\n",
      "playing\n",
      "audio\n",
      "books\n",
      ",\n",
      "and\n",
      "providing\n",
      "weather\n",
      ",\n",
      "traffic\n",
      ",\n",
      "sports\n",
      ",\n",
      "and\n",
      "other\n",
      "\n",
      "\n",
      "realtime\n",
      "information\n",
      ",\n",
      "such\n",
      "as\n",
      "news\n",
      ".\n",
      "Alexa\n",
      "can\n",
      "also\n",
      "control\n",
      "several\n",
      "smart\n",
      "devices\n",
      "using\n",
      "itself\n",
      "\n",
      "\n",
      "as\n",
      "a\n",
      "home\n",
      "automation\n",
      "system\n",
      ".\n",
      "Users\n",
      "are\n",
      "able\n",
      "to\n",
      "extend\n",
      "the\n",
      "Alexa\n",
      "capabilities\n",
      "by\n",
      "installing\n",
      "\n",
      "\n",
      "\"\n",
      "skills\n",
      "\"\n",
      "additional\n",
      "functionality\n",
      "developed\n",
      "by\n",
      "third\n",
      "-\n",
      "party\n",
      "vendors\n",
      ",\n",
      "in\n",
      "other\n",
      "settings\n",
      "more\n",
      "\n",
      "\n",
      "commonly\n",
      "called\n",
      "apps\n",
      "such\n",
      "as\n",
      "weather\n",
      "programs\n",
      "and\n",
      "audio\n",
      "features\n",
      ".\n",
      "Most\n",
      "devices\n",
      "with\n",
      "Alexa\n",
      "\n",
      "\n",
      "allow\n",
      "users\n",
      "to\n",
      "activate\n",
      "the\n",
      "device\n",
      "using\n",
      "a\n",
      "wake\n",
      "-\n",
      "word\n",
      "(\n",
      "such\n",
      "as\n",
      "Alexa\n",
      "or\n",
      "Amazon\n",
      ")\n",
      ";\n",
      "other\n",
      "devices\n",
      "\n",
      "\n",
      "(\n",
      "such\n",
      "as\n",
      "the\n",
      "Amazon\n",
      "mobile\n",
      "app\n",
      "on\n",
      "iOS\n",
      "or\n",
      "Android\n",
      "and\n",
      "Amazon\n",
      "Dash\n",
      "Wand\n",
      ")\n",
      "require\n",
      "the\n",
      "user\n",
      "to\n",
      "push\n",
      "\n",
      "\n",
      "a\n",
      "button\n",
      "to\n",
      "activate\n",
      "Alexa\n",
      "listening\n",
      "mode\n",
      ",\n",
      "although\n",
      "some\n",
      "phones\n",
      "also\n",
      "allow\n",
      "a\n",
      "user\n",
      "to\n",
      "say\n",
      "a\n",
      "\n",
      "\n",
      "command\n",
      ",\n",
      "such\n",
      "as\n",
      "\"\n",
      "Alexa\n",
      "\"\n",
      "or\n",
      "\"\n",
      "Alexa\n",
      "wake\n",
      "\"\n",
      ".\n",
      "Currently\n",
      ",\n",
      "interaction\n",
      "and\n",
      "communication\n",
      "with\n",
      "Alexa\n",
      "\n",
      "\n",
      "are\n",
      "available\n",
      "only\n",
      "in\n",
      "English\n",
      ",\n",
      "German\n",
      ",\n",
      "French\n",
      ",\n",
      "Italian\n",
      ",\n",
      "Spanish\n",
      ",\n",
      "Portuguese\n",
      ",\n",
      "Japanese\n",
      ",\n",
      "and\n",
      "Hindi\n",
      ".\n",
      "\n",
      "\n",
      "Token count is: 245\n",
      "\n",
      "Text \t\t Alpha Stop Punct\n",
      "Amazon          : True False False\n",
      "Alexa           : True False False\n",
      ",               : False False True\n",
      "also            : True True False\n",
      "known           : True False False\n",
      "simply          : True False False\n",
      "as              : True True False\n",
      "Alexa           : True False False\n",
      ",               : False False True\n",
      "is              : True True False\n",
      "a               : True True False\n",
      "virtual         : True False False\n",
      "assistant       : True False False\n",
      "AI              : True False False\n",
      "technology      : True False False\n",
      "\n",
      "               : False False False\n",
      "developed       : True False False\n",
      "by              : True True False\n",
      "Amazon          : True False False\n",
      ",               : False False True\n",
      "first           : True True False\n",
      "used            : True True False\n",
      "in              : True True False\n",
      "the             : True True False\n",
      "Amazon          : True False False\n",
      "Echo            : True False False\n",
      "smart           : True False False\n",
      "speakers        : True False False\n",
      "developed       : True False False\n",
      "by              : True True False\n",
      "Amazon          : True False False\n",
      "Lab126          : False False False\n",
      ".               : False False True\n",
      "\n",
      "               : False False False\n",
      "It              : True True False\n",
      "is              : True True False\n",
      "capable         : True False False\n",
      "of              : True True False\n",
      "voice           : True False False\n",
      "interaction     : True False False\n",
      ",               : False False True\n",
      "music           : True False False\n",
      "playback        : True False False\n",
      ",               : False False True\n",
      "making          : True False False\n",
      "to              : True True False\n",
      "-               : False False True\n",
      "do              : True True False\n",
      "lists           : True False False\n",
      ",               : False False True\n",
      "setting         : True False False\n",
      "alarms          : True False False\n",
      ",               : False False True\n",
      "\n",
      "               : False False False\n",
      "streaming       : True False False\n",
      "podcasts        : True False False\n",
      ",               : False False True\n",
      "playing         : True False False\n",
      "audio           : True False False\n",
      "books           : True False False\n",
      ",               : False False True\n",
      "and             : True True False\n",
      "providing       : True False False\n",
      "weather         : True False False\n",
      ",               : False False True\n",
      "traffic         : True False False\n",
      ",               : False False True\n",
      "sports          : True False False\n",
      ",               : False False True\n",
      "and             : True True False\n",
      "other           : True True False\n",
      "\n",
      "               : False False False\n",
      "realtime        : True False False\n",
      "information     : True False False\n",
      ",               : False False True\n",
      "such            : True True False\n",
      "as              : True True False\n",
      "news            : True False False\n",
      ".               : False False True\n",
      "Alexa           : True False False\n",
      "can             : True True False\n",
      "also            : True True False\n",
      "control         : True False False\n",
      "several         : True True False\n",
      "smart           : True False False\n",
      "devices         : True False False\n",
      "using           : True True False\n",
      "itself          : True True False\n",
      "\n",
      "               : False False False\n",
      "as              : True True False\n",
      "a               : True True False\n",
      "home            : True False False\n",
      "automation      : True False False\n",
      "system          : True False False\n",
      ".               : False False True\n",
      "Users           : True False False\n",
      "are             : True True False\n",
      "able            : True False False\n",
      "to              : True True False\n",
      "extend          : True False False\n",
      "the             : True True False\n",
      "Alexa           : True False False\n",
      "capabilities    : True False False\n",
      "by              : True True False\n",
      "installing      : True False False\n",
      "\n",
      "               : False False False\n",
      "\"               : False False True\n",
      "skills          : True False False\n",
      "\"               : False False True\n",
      "additional      : True False False\n",
      "functionality   : True False False\n",
      "developed       : True False False\n",
      "by              : True True False\n",
      "third           : True True False\n",
      "-               : False False True\n",
      "party           : True False False\n",
      "vendors         : True False False\n",
      ",               : False False True\n",
      "in              : True True False\n",
      "other           : True True False\n",
      "settings        : True False False\n",
      "more            : True True False\n",
      "\n",
      "               : False False False\n",
      "commonly        : True False False\n",
      "called          : True False False\n",
      "apps            : True False False\n",
      "such            : True True False\n",
      "as              : True True False\n",
      "weather         : True False False\n",
      "programs        : True False False\n",
      "and             : True True False\n",
      "audio           : True False False\n",
      "features        : True False False\n",
      ".               : False False True\n",
      "Most            : True True False\n",
      "devices         : True False False\n",
      "with            : True True False\n",
      "Alexa           : True False False\n",
      "\n",
      "               : False False False\n",
      "allow           : True False False\n",
      "users           : True False False\n",
      "to              : True True False\n",
      "activate        : True False False\n",
      "the             : True True False\n",
      "device          : True False False\n",
      "using           : True True False\n",
      "a               : True True False\n",
      "wake            : True False False\n",
      "-               : False False True\n",
      "word            : True False False\n",
      "(               : False False True\n",
      "such            : True True False\n",
      "as              : True True False\n",
      "Alexa           : True False False\n",
      "or              : True True False\n",
      "Amazon          : True False False\n",
      ")               : False False True\n",
      ";               : False False True\n",
      "other           : True True False\n",
      "devices         : True False False\n",
      "\n",
      "               : False False False\n",
      "(               : False False True\n",
      "such            : True True False\n",
      "as              : True True False\n",
      "the             : True True False\n",
      "Amazon          : True False False\n",
      "mobile          : True False False\n",
      "app             : True False False\n",
      "on              : True True False\n",
      "iOS             : True False False\n",
      "or              : True True False\n",
      "Android         : True False False\n",
      "and             : True True False\n",
      "Amazon          : True False False\n",
      "Dash            : True False False\n",
      "Wand            : True False False\n",
      ")               : False False True\n",
      "require         : True False False\n",
      "the             : True True False\n",
      "user            : True False False\n",
      "to              : True True False\n",
      "push            : True False False\n",
      "\n",
      "               : False False False\n",
      "a               : True True False\n",
      "button          : True False False\n",
      "to              : True True False\n",
      "activate        : True False False\n",
      "Alexa           : True False False\n",
      "listening       : True False False\n",
      "mode            : True False False\n",
      ",               : False False True\n",
      "although        : True True False\n",
      "some            : True True False\n",
      "phones          : True False False\n",
      "also            : True True False\n",
      "allow           : True False False\n",
      "a               : True True False\n",
      "user            : True False False\n",
      "to              : True True False\n",
      "say             : True True False\n",
      "a               : True True False\n",
      "\n",
      "               : False False False\n",
      "command         : True False False\n",
      ",               : False False True\n",
      "such            : True True False\n",
      "as              : True True False\n",
      "\"               : False False True\n",
      "Alexa           : True False False\n",
      "\"               : False False True\n",
      "or              : True True False\n",
      "\"               : False False True\n",
      "Alexa           : True False False\n",
      "wake            : True False False\n",
      "\"               : False False True\n",
      ".               : False False True\n",
      "Currently       : True False False\n",
      ",               : False False True\n",
      "interaction     : True False False\n",
      "and             : True True False\n",
      "communication   : True False False\n",
      "with            : True True False\n",
      "Alexa           : True False False\n",
      "\n",
      "               : False False False\n",
      "are             : True True False\n",
      "available       : True False False\n",
      "only            : True True False\n",
      "in              : True True False\n",
      "English         : True False False\n",
      ",               : False False True\n",
      "German          : True False False\n",
      ",               : False False True\n",
      "French          : True False False\n",
      ",               : False False True\n",
      "Italian         : True False False\n",
      ",               : False False True\n",
      "Spanish         : True False False\n",
      ",               : False False True\n",
      "Portuguese      : True False False\n",
      ",               : False False True\n",
      "Japanese        : True False False\n",
      ",               : False False True\n",
      "and             : True True False\n",
      "Hindi           : True False False\n",
      ".               : False False True\n",
      "\n",
      "               : False False False\n"
     ]
    }
   ],
   "source": [
    "# Checking tokens and count of tokens using spaCy\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "raw_text = \"\"\"Amazon Alexa, also known simply as Alexa, is a virtual assistant AI technology\n",
    "developed by Amazon, first used in the Amazon Echo smart speakers developed by Amazon Lab126.\n",
    "It is capable of voice interaction, music playback, making to-do lists, setting alarms,\n",
    "streaming podcasts, playing audio books, and providing weather, traffic, sports, and other\n",
    "realtime information, such as news. Alexa can also control several smart devices using itself\n",
    "as a home automation system. Users are able to extend the Alexa capabilities by installing\n",
    "\"skills\" additional functionality developed by third-party vendors, in other settings more\n",
    "commonly called apps such as weather programs and audio features. Most devices with Alexa\n",
    "allow users to activate the device using a wake-word (such as Alexa or Amazon); other devices\n",
    "(such as the Amazon mobile app on iOS or Android and Amazon Dash Wand) require the user to push\n",
    "a button to activate Alexa listening mode, although some phones also allow a user to say a\n",
    "command, such as \"Alexa\" or \"Alexa wake\". Currently, interaction and communication with Alexa\n",
    "are available only in English, German, French, Italian, Spanish, Portuguese, Japanese, and Hindi.\n",
    "\"\"\"\n",
    "\n",
    "# Process text\n",
    "text_doc = nlp(raw_text)\n",
    "\n",
    "token_count = 0\n",
    "\n",
    "for word in text_doc:\n",
    "    print(word.text)\n",
    "    token_count += 1\n",
    "\n",
    "print(\"Token count is:\", token_count)\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "print('Text',\n",
    "      '\\t\\t',\n",
    "      'Alpha',\n",
    "      'Stop',\n",
    "      'Punct')\n",
    "\n",
    "for word in text_doc:\n",
    "    print(word.text.ljust(15),\n",
    "          ':',\n",
    "          word.is_alpha,\n",
    "          word.is_stop,\n",
    "          word.is_punct)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b6648-dd49-4e26-b1b3-5891088826a4",
   "metadata": {},
   "source": [
    "## Note \n",
    "- We have a lot of tokens here . The shop words like 'it', 'was','that','to'... so on\n",
    "- There words will not give us much infomation especially for models\n",
    "- Punctuation also will not provide much info\n",
    "  - While dealing with large text files, the stop words and punctuation will be repeated high levels , misguidaing us to think they are important\n",
    "- So it is necessary to filter out the stop words\n",
    "\n",
    "## 5 stop words in spacy library\n",
    "- spacy has an inbuilt list of stop words\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dee3b9d1-c23d-4239-a27e-56a5c1d95652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['me', 'without', 'onto', 'next', 'will', 'hundred', 'neither', 'anyone', 'whither', 'herself', 'be', 'beforehand', 'n‘t', 'my', 'often', 'where', 'fifteen', 'whenever', 'amongst', 'had', 'three', 'twenty', \"'ve\", '’re', 'becoming', 'off', 'third', 'otherwise', 'are', 'keep', 'hereafter', 'mine', 'upon', 'behind', 'can', 'yourself', '‘s', '’m', 'another', 'or', 'nobody', 'along', 'her', 'what', 'nine', 'have', 'sometimes', 'say', 'somewhere', 'every', 'rather', 'seem', 'herein', 'before', 'thereby', 'noone', 'whom', 'others', 'than', 'alone', 'wherever', 'whereas', 'at', 'again', 'anyway', '‘d', 'formerly', 'during', 'below', \"'d\", 'five', 'twelve', 'get', 'make', 'of', 'each', 'but', 'through', 'part', \"'ll\", 'ca', 'how', 'i', 'more', 'among', 'see', 'n’t', 'however', 'various', 'fifty', 'its', 'meanwhile', 'their', 'towards', 'should', 'ourselves', 'all', 'namely', 'which', 'there', 'own', 'indeed', 'you', 'whereupon', 'former', 'moreover', 'really', 'as', 'may', 'regarding', 'first', 'since', 'other', 'besides', 'side', \"n't\", 'is', 'nothing', 'yourselves', \"'re\", 'either', 'until', 'to', 'ours', 'everything', 'hereby', 'except', 'could', 'not', 'just', 'perhaps', 'above', 'his', 'still', 'too', 'whence', 'whether', 'nevertheless', 'six', 'while', 'empty', 'down', 'themselves', 'whereby', 'cannot', 'hence', 'throughout', 'very', 'few', 'seemed', 'thereafter', 'thus', 'afterwards', 'amount', 'many', 'also', 'last', 'has', 'in', 'much', 'even', 'due', 'else', 'do', 'after', 'front', 'itself', 'via', 'one', \"'s\", 'from', 'because', 'why', 'into', 'up', 'mostly', 'no', 'it', 'nor', 'against', 'bottom', 'becomes', 'further', 'about', 'give', 'does', 'on', 'well', '’s', 'become', 'quite', 'move', 'might', 'would', \"'m\", 'thru', 'when', 'must', 'full', 'only', 'toward', '’ve', 'an', 'myself', 'though', 'hers', 'sixty', 'put', 'thereupon', 'if', 'two', 'same', 'us', 'out', 'unless', 'together', 'whatever', 'back', 'then', 'who', '‘re', 'eleven', 'several', 'anyhow', 'someone', 'both', 'whole', 'eight', 'was', 'some', 'take', 'therefore', 'less', 'for', 'yet', 'used', 'top', 'therein', 'latter', 'became', 'least', 'done', 'the', 'latterly', 'such', 'whoever', 'did', '’d', 'yours', 'thence', '‘m', 'made', 'none', 'whereafter', 'hereupon', 'these', 'himself', 'so', 'somehow', '‘ve', 'most', 'and', 'call', 'between', 'never', 'almost', 'beside', 'he', 'that', 'ever', 'we', 'wherein', '‘ll', 'go', 'am', 'forty', 'seems', 'everywhere', 'within', 'our', 'although', 'around', 'were', 'this', 'everyone', 'by', 'sometime', 'show', 'using', 'four', 'now', 'anything', 'she', 'here', 'serious', 'them', 'enough', 'they', 'over', 'being', 'with', 'your', 'seeming', 're', 'any', 'him', 'nowhere', 'whose', 'once', 'per', 'beyond', '’ll', 'please', 'elsewhere', 'those', 'anywhere', 'ten', 'a', 'something', 'name', 'been', 'always', 'doing', 'under', 'across', 'already']\n"
     ]
    }
   ],
   "source": [
    "## Cheacking stop words by using spacy\n",
    "\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "stopwords= spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "list_stopwords=list(stopwords)\n",
    "\n",
    "print(list_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b136e17-e592-48e7-b234-a89eb2414b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual text: Hello good !!! morning how are you. today climate is very cool\n",
      "Hello\n",
      "good\n",
      "morning\n",
      "today\n",
      "climate\n",
      "cool\n"
     ]
    }
   ],
   "source": [
    "#  Removing stop words and punctuation from text using spacy \n",
    "\n",
    "import spacy \n",
    "\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "raw_text=\"Hello good !!! morning how are you. today climate is very cool\"\n",
    "\n",
    "text_doc=nlp(raw_text)\n",
    "\n",
    "final=[\n",
    "    token\n",
    "    for token in text_doc\n",
    "    if not token.is_stop and not token.is_punct\n",
    "]\n",
    "\n",
    "print(\"Actual text:\",raw_text)\n",
    "\n",
    "for token in final:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056c4bb7-b8ee-4f16-8b94-e2d0034d651b",
   "metadata": {},
   "source": [
    "## 6 Stemming \n",
    "- General words are\n",
    "  - 'calculator'\n",
    "  - 'calculating'\n",
    "  - 'calculation'\n",
    "- We know that these words are not very distinct from each other.\n",
    "- It can be achieved through stemming\n",
    "- Stemming means reducing a word to its 'root from'\n",
    "- We can implement this by using.\n",
    "  - PorterStemmer perdefined class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8ee1af8-dd3f-4517-b549-527c6cd17e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dance ------> danc\n",
      "dances ------> danc\n",
      "dancing ------> danc\n",
      "danced ------> danc\n"
     ]
    }
   ],
   "source": [
    "# Stemming example\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps=PorterStemmer()\n",
    "\n",
    "words=['dance','dances','dancing','danced']\n",
    "\n",
    "for word in words:\n",
    "    print(word,'------>',ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce016749-7807-4711-9ee8-2461b60807e3",
   "metadata": {},
   "source": [
    "## Note \n",
    "- You can observe that some words were reduced to the base stem 'danc'\n",
    "- But the word 'danc' is not semantically correct\n",
    "- Stemming many give us root words that are not present in the dictionary\n",
    "- How to overcome this .?\n",
    "  - That's where Lemmatization comes to rescue\n",
    " \n",
    "## 7 Lemmatization \n",
    "- It is similar to stemming except that the root word is correct and always meaningful\n",
    "- There are different ways to perform lemmatization\n",
    "- we win learn lemmatization by using nltk and spacy in below examples\n",
    "\n",
    "## 8 Lemmatization using spacy \n",
    "- In spacy the token object has an attribute.lemma_which allows you to access the lemmatized version of the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb186cd3-7daf-4343-b822-9721316b88b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dance -------- dance\n",
      "dances -------- dance\n",
      "dancing -------- dance\n",
      "danced -------- dance\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization using spacy \n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "text='dance dances dancing danced'\n",
    "text_doc=nlp(text)\n",
    "\n",
    "for token in text_doc:\n",
    "    print(token.text,'--------',token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb2b476-7409-4643-9504-cdec0a4dce8b",
   "metadata": {},
   "source": [
    "## Note\n",
    "- Here all words are reduced to ' dance ' which is meanningful and just as required\n",
    "- It is highly preferres over stemming\n",
    "\n",
    "## 9 Word Frequency Analysis\n",
    "- Once we removed stop words and applied lemmatization then the next important step is , we need analyse for futher infomation about the text data\n",
    "- If any specify word repeated more times then we need to focus o that well\n",
    "- So the words which occur more frequently those are very key concepts\n",
    "- So we need to store all tokens with their frequencies separately to analyse well\n",
    "\n",
    "## 10 Counter predefined class\n",
    "- We can use counter to get the frequency of each token\n",
    "- If you provide a list to the Counter it returns a dictionary of all elements with their frequency as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cae1cbca-287f-41c0-a1c4-ef4e75fae772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: collection in /opt/miniconda3/lib/python3.13/site-packages (0.1.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1ee595b-2b1b-4022-bb27-523828bb6e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'birthday': 2, 'today': 1, 'party': 1, 'felt': 1, 'sad': 1})\n"
     ]
    }
   ],
   "source": [
    "## Counter example\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "data='it is my birthday today. I could not have a birthday party. I felt sad'\n",
    "data_doc=nlp(data)\n",
    "\n",
    "list_of_tokens=[token.text \n",
    "                for token in data_doc \n",
    "                if not token.is_stop and not token.is_punct]\n",
    "\n",
    "token_frequency=Counter(list_of_tokens)\n",
    "print(token_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1b0d58-8894-47ed-ac90-4162c852a75a",
   "metadata": {},
   "source": [
    "\n",
    "## Note \n",
    "- From above output , the word 'birthday' is most common.\n",
    "- So, it gives us an idea that the text is about a birthday\n",
    "- Let us try with another example with more large data\n",
    "- For example,\n",
    "  - If we have a text data about a particular place , and you want to know the important factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07c2a04-33cc-47bd-8bfa-4bfcd286acb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d9510dd-bb85-47f4-850c-e1fd9836faad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'\\n': 8, 'gangtok': 4, 'sikkim': 3, 'town': 2, 'indian': 2, 'city': 1, 'municipality': 1, 'capital': 1, 'largest': 1, 'state': 1, 'headquarters': 1, 'east': 1, 'district': 1, 'eastern': 1, 'himalayan': 1, 'range': 1, 'elevation': 1, '1,650': 1, 'population': 1, '100000': 1, 'different': 1, 'ethnicities': 1, 'bhutia': 1, 'lepchas': 1, 'gorkhas': 1, 'higher': 1, 'peaks': 1, 'himalaya': 1, 'year': 1, 'round': 1, 'mild': 1, 'temperate': 1, 'climate': 1, 'centre': 1, 'tourism': 1, 'industry': 1, 'rose': 1, 'prominence': 1, 'popular': 1, 'buddhist': 1, 'pilgrimage': 1, 'site': 1, 'construction': 1, 'enchey': 1, 'monastery': 1, '1840': 1})\n",
      "Counter({'\\n': 8, 'Gangtok': 4, 'Sikkim': 3, 'town': 2, 'Indian': 2, 'city': 1, 'municipality': 1, 'capital': 1, 'largest': 1, 'state': 1, 'headquarters': 1, 'East': 1, 'district': 1, 'eastern': 1, 'Himalayan': 1, 'range': 1, 'elevation': 1, '1,650': 1, 'population': 1, '100000': 1, 'different': 1, 'ethnicities': 1, 'Bhutia': 1, 'Lepchas': 1, 'Gorkhas': 1, 'higher': 1, 'peaks': 1, 'Himalaya': 1, 'year': 1, 'round': 1, 'mild': 1, 'temperate': 1, 'climate': 1, 'centre': 1, 'tourism': 1, 'industry': 1, 'rose': 1, 'prominence': 1, 'popular': 1, 'Buddhist': 1, 'pilgrimage': 1, 'site': 1, 'construction': 1, 'Enchey': 1, 'Monastery': 1, '1840': 1})\n"
     ]
    }
   ],
   "source": [
    "# Counter example\n",
    "\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "longtext = \"\"\"Gangtok is a city, municipality, the capital and the largest town of\n",
    "the Indian state of Sikkim. It is also the headquarters of the East Sikkim district.\n",
    "Gangtok is in the eastern Himalayan range, at an elevation of 1,650.\n",
    "The town's population of 100000 are from different ethnicities such as Bhutia,\n",
    "Lepchas and Indian Gorkhas. Within the higher peaks of the Himalaya and with a\n",
    "year-round mild temperate climate, Gangtok is at the centre of Sikkim's tourism industry.\n",
    "Gangtok rose to prominence as a popular Buddhist pilgrimage site after the\n",
    "construction of the Enchey Monastery in 1840.\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(longtext)\n",
    "\n",
    "tokens = [\n",
    "    token.text.lower()\n",
    "    for token in doc\n",
    "    if not token.is_stop and not token.is_punct\n",
    "]\n",
    "\n",
    "token_frequency = Counter(tokens)\n",
    "print(token_frequency)\n",
    "\n",
    "long_text=nlp(longtext)\n",
    "\n",
    "list_of_tokens=[token.text \n",
    "                for token in long_text \n",
    "                if not token.is_stop and not token.is_punct]\n",
    "\n",
    "token_frequency=Counter(list_of_tokens)\n",
    "print(token_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "247eea24-af04-4beb-9c0f-f059f6841381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'gangtok': 18, 'city': 10, 'india': 8, '\\n\\n': 5, 'sikkim': 4, 'cultural': 4, 'people': 4, 'located': 3, 'capital': 3, 'beauty': 3, 'makes': 3, 'popular': 3, 'natural': 3, 'monasteries': 3, 'education': 3, 'cities': 3, 'state': 2, 'peaceful': 2, 'offers': 2, 'tourist': 2, 'year': 2, 'communities': 2, 'food': 2, 'unique': 2, 'includes': 2, 'street': 2, 'common': 2, 'seen': 2, 'monastery': 2, 'tok': 2, 'spiritual': 2, 'places': 2, 'prayer': 2, 'calm': 2, 'gained': 2, 'importance': 2, 'important': 2, 'tourism': 2, 'tsomgo': 2, 'lake': 2, 'nathula': 2, 'pass': 2, 'mg': 2, 'marg': 2, 'major': 2, 'historical': 2, 'remains': 2, 'attracts': 2, 'transportation': 2, 'railway': 2, 'nearest': 2, 'airport': 2, ' ': 1, 'beautiful': 1, 'northeastern': 1, 'known': 1, 'scenic': 1, 'environment': 1, 'rich': 1, 'heritage': 1, 'lies': 1, 'eastern': 1, 'himalayan': 1, 'range': 1, 'breathtaking': 1, 'views': 1, 'snow': 1, 'covered': 1, 'mountains': 1, 'green': 1, 'valleys': 1, 'winding': 1, 'roads': 1, 'pleasant': 1, 'climate': 1, 'destination': 1, 'different': 1, 'parts': 1, 'world': 1, 'visit': 1, 'experience': 1, 'local': 1, 'traditions': 1, 'population': 1, 'consists': 1, 'belonging': 1, 'ethnic': 1, 'groups': 1, 'lepchas': 1, 'bhutias': 1, 'nepali': 1, 'live': 1, 'peacefully': 1, 'contribute': 1, 'diversity': 1, 'festivals': 1, 'like': 1, 'losar': 1, 'saga': 1, 'dawa': 1, 'diwali': 1, 'dasain': 1, 'celebrated': 1, 'great': 1, 'enthusiasm': 1, 'culture': 1, 'dishes': 1, 'momos': 1, 'thukpa': 1, 'phagshapa': 1, 'traditional': 1, 'sikkimese': 1, 'meals': 1, 'stalls': 1, 'attract': 1, 'locals': 1, 'tourists': 1, 'strong': 1, 'buddhist': 1, 'influence': 1, 'religious': 1, 'practices': 1, 'enchey': 1, 'rumtek': 1, 'ganesh': 1, 'famous': 1, 'serve': 1, 'worship': 1, 'centers': 1, 'learning': 1, 'meditation': 1, 'flags': 1, 'wheels': 1, 'chanting': 1, 'monks': 1, 'sights': 1, 'atmosphere': 1, 'historically': 1, 'small': 1, 'village': 1, 'nineteenth': 1, 'century': 1, 'developed': 1, 'trade': 1, 'route': 1, 'tibet': 1, 'independence': 1, 'remained': 1, 'monarchy': 1, 'merged': 1, '1975': 1, 'official': 1, 'indian': 1, 'steady': 1, 'development': 1, 'infrastructure': 1, 'plays': 1, 'role': 1, 'schools': 1, 'colleges': 1, 'institutes': 1, 'offering': 1, 'quality': 1, 'students': 1, 'nearby': 1, 'regions': 1, 'come': 1, 'pursue': 1, 'higher': 1, 'studies': 1, 'focuses': 1, 'environmental': 1, 'sustainability': 1, 'cleanliness': 1, 'plastic': 1, 'usage': 1, 'restricted': 1, 'efforts': 1, 'preserve': 1, 'ecosystem': 1, 'regarded': 1, 'cleanest': 1, 'backbone': 1, 'economy': 1, 'attractions': 1, 'include': 1, 'hanuman': 1, 'pedestrian': 1, 'filled': 1, 'shops': 1, 'cafes': 1, 'restaurants': 1, 'making': 1, 'favorite': 1, 'hangout': 1, 'spot': 1, 'near': 1, 'china': 1, 'border': 1, 'attraction': 1, 'strategic': 1, 'crystal': 1, 'clear': 1, 'water': 1, 'frozen': 1, 'winter': 1, 'visitors': 1, 'mainly': 1, 'taxis': 1, 'shared': 1, 'vehicles': 1, 'hilly': 1, 'terrain': 1, 'connectivity': 1, 'difficult': 1, 'station': 1, 'new': 1, 'jalpaiguri': 1, 'west': 1, 'bengal': 1, 'pakyong': 1, 'despite': 1, 'challenges': 1, 'connected': 1, 'road': 1, 'networks': 1, 'lifestyle': 1, 'compared': 1, 'metropolitan': 1, 'value': 1, 'nature': 1, 'community': 1, 'balance': 1, 'life': 1, 'slower': 1, 'pace': 1, 'living': 1, 'looking': 1, 'peace': 1, 'simplicity': 1, 'increasing': 1, 'modernization': 1, 'continues': 1, 'maintain': 1, 'roots': 1, 'adapting': 1, 'change': 1, 'combination': 1, 'harmony': 1, 'significance': 1, 'charming': 1})\n",
      "Counter({'Gangtok': 18, 'city': 10, 'India': 8, '\\n\\n': 5, 'Sikkim': 4, 'cultural': 4, 'located': 3, 'capital': 3, 'beauty': 3, 'makes': 3, 'natural': 3, 'monasteries': 3, 'cities': 3, 'state': 2, 'peaceful': 2, 'offers': 2, 'popular': 2, 'tourist': 2, 'year': 2, 'People': 2, 'people': 2, 'communities': 2, 'food': 2, 'unique': 2, 'includes': 2, 'common': 2, 'seen': 2, 'Monastery': 2, 'Tok': 2, 'spiritual': 2, 'places': 2, 'calm': 2, 'gained': 2, 'importance': 2, 'important': 2, 'education': 2, 'Tsomgo': 2, 'Lake': 2, 'Nathula': 2, 'Pass': 2, 'MG': 2, 'Marg': 2, 'major': 2, 'historical': 2, 'remains': 2, 'attracts': 2, 'railway': 2, 'nearest': 2, ' ': 1, 'beautiful': 1, 'northeastern': 1, 'known': 1, 'scenic': 1, 'environment': 1, 'rich': 1, 'heritage': 1, 'lies': 1, 'eastern': 1, 'Himalayan': 1, 'range': 1, 'breathtaking': 1, 'views': 1, 'snow': 1, 'covered': 1, 'mountains': 1, 'green': 1, 'valleys': 1, 'winding': 1, 'roads': 1, 'pleasant': 1, 'climate': 1, 'destination': 1, 'different': 1, 'parts': 1, 'world': 1, 'visit': 1, 'experience': 1, 'local': 1, 'traditions': 1, 'population': 1, 'consists': 1, 'belonging': 1, 'ethnic': 1, 'groups': 1, 'Lepchas': 1, 'Bhutias': 1, 'Nepali': 1, 'live': 1, 'peacefully': 1, 'contribute': 1, 'diversity': 1, 'Festivals': 1, 'like': 1, 'Losar': 1, 'Saga': 1, 'Dawa': 1, 'Diwali': 1, 'Dasain': 1, 'celebrated': 1, 'great': 1, 'enthusiasm': 1, 'culture': 1, 'dishes': 1, 'momos': 1, 'thukpa': 1, 'phagshapa': 1, 'traditional': 1, 'Sikkimese': 1, 'meals': 1, 'Street': 1, 'stalls': 1, 'attract': 1, 'locals': 1, 'tourists': 1, 'strong': 1, 'Buddhist': 1, 'influence': 1, 'religious': 1, 'practices': 1, 'Enchey': 1, 'Rumtek': 1, 'Ganesh': 1, 'famous': 1, 'serve': 1, 'worship': 1, 'centers': 1, 'learning': 1, 'meditation': 1, 'Prayer': 1, 'flags': 1, 'prayer': 1, 'wheels': 1, 'chanting': 1, 'monks': 1, 'sights': 1, 'atmosphere': 1, 'Historically': 1, 'small': 1, 'village': 1, 'nineteenth': 1, 'century': 1, 'developed': 1, 'trade': 1, 'route': 1, 'Tibet': 1, 'independence': 1, 'remained': 1, 'monarchy': 1, 'merged': 1, '1975': 1, 'official': 1, 'Indian': 1, 'steady': 1, 'development': 1, 'infrastructure': 1, 'tourism': 1, 'Education': 1, 'plays': 1, 'role': 1, 'schools': 1, 'colleges': 1, 'institutes': 1, 'offering': 1, 'quality': 1, 'Students': 1, 'nearby': 1, 'regions': 1, 'come': 1, 'pursue': 1, 'higher': 1, 'studies': 1, 'focuses': 1, 'environmental': 1, 'sustainability': 1, 'cleanliness': 1, 'Plastic': 1, 'usage': 1, 'restricted': 1, 'efforts': 1, 'preserve': 1, 'ecosystem': 1, 'regarded': 1, 'cleanest': 1, 'Tourism': 1, 'backbone': 1, 'economy': 1, 'Popular': 1, 'attractions': 1, 'include': 1, 'Hanuman': 1, 'pedestrian': 1, 'street': 1, 'filled': 1, 'shops': 1, 'cafes': 1, 'restaurants': 1, 'making': 1, 'favorite': 1, 'hangout': 1, 'spot': 1, 'near': 1, 'China': 1, 'border': 1, 'attraction': 1, 'strategic': 1, 'crystal': 1, 'clear': 1, 'water': 1, 'frozen': 1, 'winter': 1, 'visitors': 1, 'Transportation': 1, 'mainly': 1, 'taxis': 1, 'shared': 1, 'vehicles': 1, 'hilly': 1, 'terrain': 1, 'connectivity': 1, 'difficult': 1, 'station': 1, 'New': 1, 'Jalpaiguri': 1, 'West': 1, 'Bengal': 1, 'airport': 1, 'Pakyong': 1, 'Airport': 1, 'Despite': 1, 'transportation': 1, 'challenges': 1, 'connected': 1, 'road': 1, 'networks': 1, 'lifestyle': 1, 'compared': 1, 'metropolitan': 1, 'value': 1, 'nature': 1, 'community': 1, 'balance': 1, 'life': 1, 'slower': 1, 'pace': 1, 'living': 1, 'looking': 1, 'peace': 1, 'simplicity': 1, 'increasing': 1, 'modernization': 1, 'continues': 1, 'maintain': 1, 'roots': 1, 'adapting': 1, 'change': 1, 'combination': 1, 'harmony': 1, 'significance': 1, 'charming': 1})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "longtext=\"\"\" Gangtok is a beautiful city located in the northeastern part of India and it is the capital of the state of Sikkim. The city is known for its scenic beauty, peaceful environment, and rich cultural heritage. Gangtok lies in the eastern Himalayan range and offers breathtaking views of snow covered mountains, green valleys, and winding roads. The pleasant climate of Gangtok makes it a popular tourist destination throughout the year. People from different parts of India and the world visit Gangtok to experience its natural beauty, monasteries, and local traditions.\n",
    "\n",
    "The population of Gangtok consists of people belonging to various ethnic groups such as Lepchas, Bhutias, and Nepali communities. These communities live together peacefully and contribute to the cultural diversity of the city. Festivals like Losar, Saga Dawa, Diwali, and Dasain are celebrated with great enthusiasm. The food culture of Gangtok is also unique and includes popular dishes such as momos, thukpa, phagshapa, and traditional Sikkimese meals. Street food stalls are common in the city and attract both locals and tourists.\n",
    "\n",
    "Gangtok has a strong Buddhist influence, which can be seen in its monasteries and religious practices. The Enchey Monastery, Rumtek Monastery, and Ganesh Tok are some of the most famous spiritual places in the city. These monasteries not only serve as places of worship but also as centers of learning and meditation. Prayer flags, prayer wheels, and chanting monks are common sights that give Gangtok a calm and spiritual atmosphere. Historically, Gangtok was a small village that gained importance after becoming the capital of Sikkim. During the nineteenth century, it developed as an important trade route between India and Tibet. After India gained independence, Sikkim remained a monarchy until it merged with India in 1975. Gangtok then became the official capital of the Indian state of Sikkim. Since then, the city has seen steady development in infrastructure, education, and tourism.\n",
    "\n",
    "Education plays an important role in the city, with several schools, colleges, and institutes offering quality education. Students from nearby regions come to Gangtok to pursue higher studies. The city also focuses on environmental sustainability and cleanliness. Plastic usage is restricted, and efforts are made to preserve the natural ecosystem. Gangtok is often regarded as one of the cleanest cities in India.\n",
    "\n",
    "Tourism is the backbone of Gangtok’s economy. Popular tourist attractions include Tsomgo Lake, Nathula Pass, MG Marg, and Hanuman Tok. MG Marg is a pedestrian-only street filled with shops, cafes, and restaurants, making it a favorite hangout spot. Nathula Pass, located near the India-China border, is a major attraction due to its historical and strategic importance. Tsomgo Lake, with its crystal clear water, remains frozen during winter and attracts visitors throughout the year. Transportation in Gangtok mainly includes taxis and shared vehicles, as the hilly terrain makes railway connectivity difficult. The nearest railway station is located in New Jalpaiguri, West Bengal, while the nearest airport is Pakyong Airport. Despite transportation challenges, Gangtok remains well connected to major cities through road networks.\n",
    "\n",
    "The lifestyle in Gangtok is calm and peaceful compared to metropolitan cities. People value nature, community, and balance in life. The city offers a slower pace of living, which attracts people looking for peace and simplicity. With increasing modernization, Gangtok continues to maintain its cultural roots while adapting to change. The combination of natural beauty, cultural harmony, and historical significance makes Gangtok a unique and charming city in India. \"\"\"\n",
    "\n",
    "doc = nlp(longtext)\n",
    "\n",
    "tokens = [\n",
    "    token.text.lower()\n",
    "    for token in doc\n",
    "    if not token.is_stop and not token.is_punct\n",
    "]\n",
    "\n",
    "token_frequency = Counter(tokens)\n",
    "print(token_frequency)\n",
    "\n",
    "long_text=nlp(longtext)\n",
    "\n",
    "list_of_tokens=[token.text \n",
    "                for token in long_text \n",
    "                if not token.is_stop and not token.is_punct]\n",
    "\n",
    "token_frequency=Counter(list_of_tokens)\n",
    "print(token_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79ae5515-5977-4ee5-8e78-77da60a547b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'gangtok': 18, 'city': 10, 'india': 8, '\\n\\n': 5, 'sikkim': 4, 'cultural': 4, 'people': 4, 'located': 3, 'capital': 3, 'beauty': 3, 'makes': 3, 'popular': 3, 'natural': 3, 'monasteries': 3, 'education': 3, 'cities': 3, 'state': 2, 'peaceful': 2, 'offers': 2, 'tourist': 2, 'year': 2, 'communities': 2, 'food': 2, 'unique': 2, 'includes': 2, 'street': 2, 'common': 2, 'seen': 2, 'monastery': 2, 'tok': 2, 'spiritual': 2, 'places': 2, 'prayer': 2, 'calm': 2, 'gained': 2, 'importance': 2, 'important': 2, 'tourism': 2, 'tsomgo': 2, 'lake': 2, 'nathula': 2, 'pass': 2, 'mg': 2, 'marg': 2, 'major': 2, 'historical': 2, 'remains': 2, 'attracts': 2, 'transportation': 2, 'railway': 2, 'nearest': 2, 'airport': 2, ' ': 1, 'beautiful': 1, 'northeastern': 1, 'known': 1, 'scenic': 1, 'environment': 1, 'rich': 1, 'heritage': 1, 'lies': 1, 'eastern': 1, 'himalayan': 1, 'range': 1, 'breathtaking': 1, 'views': 1, 'snow': 1, 'covered': 1, 'mountains': 1, 'green': 1, 'valleys': 1, 'winding': 1, 'roads': 1, 'pleasant': 1, 'climate': 1, 'destination': 1, 'different': 1, 'parts': 1, 'world': 1, 'visit': 1, 'experience': 1, 'local': 1, 'traditions': 1, 'population': 1, 'consists': 1, 'belonging': 1, 'ethnic': 1, 'groups': 1, 'lepchas': 1, 'bhutias': 1, 'nepali': 1, 'live': 1, 'peacefully': 1, 'contribute': 1, 'diversity': 1, 'festivals': 1, 'like': 1, 'losar': 1, 'saga': 1, 'dawa': 1, 'diwali': 1, 'dasain': 1, 'celebrated': 1, 'great': 1, 'enthusiasm': 1, 'culture': 1, 'dishes': 1, 'momos': 1, 'thukpa': 1, 'phagshapa': 1, 'traditional': 1, 'sikkimese': 1, 'meals': 1, 'stalls': 1, 'attract': 1, 'locals': 1, 'tourists': 1, 'strong': 1, 'buddhist': 1, 'influence': 1, 'religious': 1, 'practices': 1, 'enchey': 1, 'rumtek': 1, 'ganesh': 1, 'famous': 1, 'serve': 1, 'worship': 1, 'centers': 1, 'learning': 1, 'meditation': 1, 'flags': 1, 'wheels': 1, 'chanting': 1, 'monks': 1, 'sights': 1, 'atmosphere': 1, 'historically': 1, 'small': 1, 'village': 1, 'nineteenth': 1, 'century': 1, 'developed': 1, 'trade': 1, 'route': 1, 'tibet': 1, 'independence': 1, 'remained': 1, 'monarchy': 1, 'merged': 1, '1975': 1, 'official': 1, 'indian': 1, 'steady': 1, 'development': 1, 'infrastructure': 1, 'plays': 1, 'role': 1, 'schools': 1, 'colleges': 1, 'institutes': 1, 'offering': 1, 'quality': 1, 'students': 1, 'nearby': 1, 'regions': 1, 'come': 1, 'pursue': 1, 'higher': 1, 'studies': 1, 'focuses': 1, 'environmental': 1, 'sustainability': 1, 'cleanliness': 1, 'plastic': 1, 'usage': 1, 'restricted': 1, 'efforts': 1, 'preserve': 1, 'ecosystem': 1, 'regarded': 1, 'cleanest': 1, 'backbone': 1, 'economy': 1, 'attractions': 1, 'include': 1, 'hanuman': 1, 'pedestrian': 1, 'filled': 1, 'shops': 1, 'cafes': 1, 'restaurants': 1, 'making': 1, 'favorite': 1, 'hangout': 1, 'spot': 1, 'near': 1, 'china': 1, 'border': 1, 'attraction': 1, 'strategic': 1, 'crystal': 1, 'clear': 1, 'water': 1, 'frozen': 1, 'winter': 1, 'visitors': 1, 'mainly': 1, 'taxis': 1, 'shared': 1, 'vehicles': 1, 'hilly': 1, 'terrain': 1, 'connectivity': 1, 'difficult': 1, 'station': 1, 'new': 1, 'jalpaiguri': 1, 'west': 1, 'bengal': 1, 'pakyong': 1, 'despite': 1, 'challenges': 1, 'connected': 1, 'road': 1, 'networks': 1, 'lifestyle': 1, 'compared': 1, 'metropolitan': 1, 'value': 1, 'nature': 1, 'community': 1, 'balance': 1, 'life': 1, 'slower': 1, 'pace': 1, 'living': 1, 'looking': 1, 'peace': 1, 'simplicity': 1, 'increasing': 1, 'modernization': 1, 'continues': 1, 'maintain': 1, 'roots': 1, 'adapting': 1, 'change': 1, 'combination': 1, 'harmony': 1, 'significance': 1, 'charming': 1})\n",
      "[('Gangtok', 18), ('city', 10), ('India', 8), ('\\n\\n', 5), ('Sikkim', 4), ('cultural', 4)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# most common 6 in the paregraph \n",
    "\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "longtext=\"\"\" Gangtok is a beautiful city located in the northeastern part of India and it is the capital of the state of Sikkim. The city is known for its scenic beauty, peaceful environment, and rich cultural heritage. Gangtok lies in the eastern Himalayan range and offers breathtaking views of snow covered mountains, green valleys, and winding roads. The pleasant climate of Gangtok makes it a popular tourist destination throughout the year. People from different parts of India and the world visit Gangtok to experience its natural beauty, monasteries, and local traditions.\n",
    "\n",
    "The population of Gangtok consists of people belonging to various ethnic groups such as Lepchas, Bhutias, and Nepali communities. These communities live together peacefully and contribute to the cultural diversity of the city. Festivals like Losar, Saga Dawa, Diwali, and Dasain are celebrated with great enthusiasm. The food culture of Gangtok is also unique and includes popular dishes such as momos, thukpa, phagshapa, and traditional Sikkimese meals. Street food stalls are common in the city and attract both locals and tourists.\n",
    "\n",
    "Gangtok has a strong Buddhist influence, which can be seen in its monasteries and religious practices. The Enchey Monastery, Rumtek Monastery, and Ganesh Tok are some of the most famous spiritual places in the city. These monasteries not only serve as places of worship but also as centers of learning and meditation. Prayer flags, prayer wheels, and chanting monks are common sights that give Gangtok a calm and spiritual atmosphere. Historically, Gangtok was a small village that gained importance after becoming the capital of Sikkim. During the nineteenth century, it developed as an important trade route between India and Tibet. After India gained independence, Sikkim remained a monarchy until it merged with India in 1975. Gangtok then became the official capital of the Indian state of Sikkim. Since then, the city has seen steady development in infrastructure, education, and tourism.\n",
    "\n",
    "Education plays an important role in the city, with several schools, colleges, and institutes offering quality education. Students from nearby regions come to Gangtok to pursue higher studies. The city also focuses on environmental sustainability and cleanliness. Plastic usage is restricted, and efforts are made to preserve the natural ecosystem. Gangtok is often regarded as one of the cleanest cities in India.\n",
    "\n",
    "Tourism is the backbone of Gangtok’s economy. Popular tourist attractions include Tsomgo Lake, Nathula Pass, MG Marg, and Hanuman Tok. MG Marg is a pedestrian-only street filled with shops, cafes, and restaurants, making it a favorite hangout spot. Nathula Pass, located near the India-China border, is a major attraction due to its historical and strategic importance. Tsomgo Lake, with its crystal clear water, remains frozen during winter and attracts visitors throughout the year. Transportation in Gangtok mainly includes taxis and shared vehicles, as the hilly terrain makes railway connectivity difficult. The nearest railway station is located in New Jalpaiguri, West Bengal, while the nearest airport is Pakyong Airport. Despite transportation challenges, Gangtok remains well connected to major cities through road networks.\n",
    "\n",
    "The lifestyle in Gangtok is calm and peaceful compared to metropolitan cities. People value nature, community, and balance in life. The city offers a slower pace of living, which attracts people looking for peace and simplicity. With increasing modernization, Gangtok continues to maintain its cultural roots while adapting to change. The combination of natural beauty, cultural harmony, and historical significance makes Gangtok a unique and charming city in India. \"\"\"\n",
    "\n",
    "doc = nlp(longtext)\n",
    "\n",
    "tokens = [\n",
    "    token.text.lower()\n",
    "    for token in doc\n",
    "    if not token.is_stop and not token.is_punct\n",
    "]\n",
    "\n",
    "token_frequency = Counter(tokens)\n",
    "print(token_frequency)\n",
    "\n",
    "long_text=nlp(longtext)\n",
    "\n",
    "list_of_tokens=[token.text \n",
    "                for token in long_text \n",
    "                if not token.is_stop and not token.is_punct]\n",
    "\n",
    "token_frequency=Counter(list_of_tokens)\n",
    "\n",
    "most_frequent_tokens=token_frequency.most_common(6)\n",
    "print(most_frequent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7675e4-2a7f-4c70-bba6-3837976a7bbb",
   "metadata": {},
   "source": [
    "## Note\n",
    "- We can see that the keywords are gangtok, sikkkim, Indian and so on.\n",
    "- It gives us an idea of the text to start with.\n",
    "\n",
    "## 11 Part of Speech Tagging \n",
    "- Each word has its own role in a sentence\n",
    "- For example.\n",
    "  - 'Daniel is dancing\n",
    "  - Daniel is the person or 'Noun' and dancing is the action performed by him\n",
    "  - So it is a 'verb'\n",
    "  - Likewise each word can be classified\n",
    "  - This is referred as POS or Part of Speech Tagging\n",
    "\n",
    "## 12 Parts of Speech \n",
    "- None\n",
    "- Verb\n",
    "- Adjective\n",
    "- Pronoun\n",
    "- Adverb\n",
    "- Preposition\n",
    "- Conjunction\n",
    "- Interjection and So on\n",
    "\n",
    "## 13 Pos by using spacy\n",
    "- POS can be implemented using both spaCy and nltk\n",
    "- First, let us see the method using spaCy\n",
    "- In spaCy, the POS tags are present in the attribute of Token object\n",
    "- We can acess the POS tag of particular token through the token.pos_attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da3ca257-faab-41d3-b318-863464b9a838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daniel     ------ univ_pos_t.PROPN\n",
      "is         ------ univ_pos_t.AUX\n",
      "singing    ------ univ_pos_t.VERB\n",
      "loudly     ------ univ_pos_t.ADV\n",
      "and        ------ univ_pos_t.CCONJ\n",
      "his        ------ univ_pos_t.PRON\n",
      "roommates  ------ univ_pos_t.NOUN\n",
      "are        ------ univ_pos_t.AUX\n",
      "enjoying   ------ univ_pos_t.VERB\n",
      "too        ------ univ_pos_t.ADV\n"
     ]
    }
   ],
   "source": [
    "# POS Example\n",
    "\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "text='Daniel is singing loudly and his roommates are enjoying too '\n",
    "text_doc=nlp(text)\n",
    "\n",
    "for word in text_doc:\n",
    "    print(word.text.ljust(10),'------',word.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b822eebe-972f-4d8d-81e8-8e04b27508c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is\n",
      "are\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Filltering\n",
    "\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "text='Daniel is singing loudly and his roommates are enjoying too '\n",
    "text_doc=nlp(text)\n",
    "\n",
    "for word in text_doc:\n",
    "    if word.pos_ == 'AUX':\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b752950-ea9b-4844-96d3-5e15d6281438",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 2) (2241946580.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    Even as the UK became the first country to grant emergency approval to Pfizer-BioNTech's Covid-19 vaccine, the US firm said it is \"committed\" to engaging with the Indian government to \"explore opportunities\" to roll it out here. The vaccine will be a challenge for India as it requires storage at -70 degrees, experts say.\u001b[0m\n\u001b[0m                                                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 2)\n"
     ]
    }
   ],
   "source": [
    "covid.txt|\n",
    "Even as the UK became the first country to grant emergency approval to Pfizer-BioNTech's Covid-19 vaccine, the US firm said it is \"committed\" to engaging with the Indian government to \"explore opportunities\" to roll it out here. The vaccine will be a challenge for India as it requires storage at -70 degrees, experts say.\n",
    "Pfizer spokeswoman Roma Nair told TOI: \"We are committed to advance our dialogue with the Indian government. We are working with governments across the world to understand the infrastructure requirements of each country and we have logistical plans in place. We are confident the rollout can be managed in India.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fd22ea6-e6f3-415d-b050-f2226ee6cd9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Even as the UK became the first country to grant emergency approval to\n",
       "Pfizer-BioNTech’s Covid-19 vaccine, the US firm said it is “committed”\n",
       "to engaging with the Indian government to “explore opportunities” to\n",
       "roll it out here. The vaccine will be a challenge for India as it\n",
       "requires storage at -70 degrees, experts say.\n",
       "\n",
       "Pfizer spokeswoman Roma Nair told TOI: “We are committed to advance our\n",
       "dialogue with the Indian government. We are working with governments\n",
       "across the world to understand the infrastructure requirements of each\n",
       "country and we have logistical plans in place. We are confident the\n",
       "rollout can be managed in India.”"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing the text by using spacy\n",
    "\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "with open('/Users/apple/Downloads/covid.txt') as file:\n",
    "    robot_text=file.read()\n",
    "\n",
    "roboat_doc=nlp(robot_text)\n",
    "\n",
    "roboat_doc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f065fc6-6d34-438d-8d7b-3e8fe197ac04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UK, country, grant, emergency, approval, \n",
      ", Pfizer, BioNTech, Covid-19, vaccine, firm, said, committed, \n",
      ", engaging, Indian, government, explore, opportunities, \n",
      ", roll, vaccine, challenge, India, \n",
      ", requires, storage, -70, degrees, experts, \n",
      "\n",
      ", Pfizer, spokeswoman, Roma, Nair, told, TOI, committed, advance, \n",
      ", dialogue, Indian, government, working, governments, \n",
      ", world, understand, infrastructure, requirements, \n",
      ", country, logistical, plans, place, confident, \n",
      ", rollout, managed, India, \n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the text by using spacy\n",
    "\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "with open('/Users/apple/Downloads/covid.txt') as file:\n",
    "    robot_text=file.read()\n",
    "\n",
    "roboat_doc=nlp(robot_text)\n",
    "\n",
    "roboat_doc = [\n",
    "    word\n",
    "    for word in roboat_doc\n",
    "    if not word.is_stop and not word.is_punct\n",
    "]\n",
    "\n",
    "print(roboat_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80ec750f-9192-4aab-9228-28275197b8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UK\n",
      "country\n",
      "grant\n",
      "emergency\n",
      "approval\n",
      "\n",
      "\n",
      "Pfizer\n",
      "BioNTech\n",
      "Covid-19\n",
      "vaccine\n",
      "firm\n",
      "said\n",
      "committed\n",
      "\n",
      "\n",
      "engaging\n",
      "Indian\n",
      "government\n",
      "explore\n",
      "opportunities\n",
      "\n",
      "\n",
      "roll\n",
      "vaccine\n",
      "challenge\n",
      "India\n",
      "\n",
      "\n",
      "requires\n",
      "storage\n",
      "-70\n",
      "degrees\n",
      "experts\n",
      "\n",
      "\n",
      "\n",
      "Pfizer\n",
      "spokeswoman\n",
      "Roma\n",
      "Nair\n",
      "told\n",
      "TOI\n",
      "committed\n",
      "advance\n",
      "\n",
      "\n",
      "dialogue\n",
      "Indian\n",
      "government\n",
      "working\n",
      "governments\n",
      "\n",
      "\n",
      "world\n",
      "understand\n",
      "infrastructure\n",
      "requirements\n",
      "\n",
      "\n",
      "country\n",
      "logistical\n",
      "plans\n",
      "place\n",
      "confident\n",
      "\n",
      "\n",
      "rollout\n",
      "managed\n",
      "India\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "with open('/Users/apple/Downloads/covid.txt') as file:\n",
    "    robot_text=file.read()\n",
    "\n",
    "roboat_doc=nlp(robot_text)\n",
    "\n",
    "roboat_doc = [\n",
    "    word\n",
    "    for word in roboat_doc\n",
    "    if not word.is_stop and not word.is_punct\n",
    "]\n",
    "\n",
    "nouns=[]\n",
    "verbs=[]\n",
    "\n",
    "for word in roboat_doc:\n",
    "    print(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef6b6b9b-5f3a-4af4-8086-fd3e8892b53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UK =========== PROPN\n",
      "country =========== NOUN\n",
      "grant =========== VERB\n",
      "emergency =========== NOUN\n",
      "approval =========== NOUN\n",
      "\n",
      " =========== SPACE\n",
      "Pfizer =========== NOUN\n",
      "BioNTech =========== NOUN\n",
      "Covid-19 =========== PROPN\n",
      "vaccine =========== NOUN\n",
      "firm =========== NOUN\n",
      "said =========== VERB\n",
      "committed =========== ADJ\n",
      "\n",
      " =========== SPACE\n",
      "engaging =========== VERB\n",
      "Indian =========== ADJ\n",
      "government =========== NOUN\n",
      "explore =========== VERB\n",
      "opportunities =========== NOUN\n",
      "\n",
      " =========== SPACE\n",
      "roll =========== VERB\n",
      "vaccine =========== NOUN\n",
      "challenge =========== NOUN\n",
      "India =========== PROPN\n",
      "\n",
      " =========== SPACE\n",
      "requires =========== VERB\n",
      "storage =========== NOUN\n",
      "-70 =========== PROPN\n",
      "degrees =========== NOUN\n",
      "experts =========== NOUN\n",
      "\n",
      "\n",
      " =========== SPACE\n",
      "Pfizer =========== PROPN\n",
      "spokeswoman =========== NOUN\n",
      "Roma =========== PROPN\n",
      "Nair =========== PROPN\n",
      "told =========== VERB\n",
      "TOI =========== PROPN\n",
      "committed =========== VERB\n",
      "advance =========== VERB\n",
      "\n",
      " =========== SPACE\n",
      "dialogue =========== NOUN\n",
      "Indian =========== ADJ\n",
      "government =========== NOUN\n",
      "working =========== VERB\n",
      "governments =========== NOUN\n",
      "\n",
      " =========== SPACE\n",
      "world =========== NOUN\n",
      "understand =========== VERB\n",
      "infrastructure =========== NOUN\n",
      "requirements =========== NOUN\n",
      "\n",
      " =========== SPACE\n",
      "country =========== NOUN\n",
      "logistical =========== ADJ\n",
      "plans =========== NOUN\n",
      "place =========== NOUN\n",
      "confident =========== ADJ\n",
      "\n",
      " =========== SPACE\n",
      "rollout =========== NOUN\n",
      "managed =========== VERB\n",
      "India =========== PROPN\n",
      "\n",
      " =========== SPACE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "with open('/Users/apple/Downloads/covid.txt') as file:\n",
    "    robot_text=file.read()\n",
    "\n",
    "roboat_doc=nlp(robot_text)\n",
    "\n",
    "roboat_doc = [\n",
    "    word\n",
    "    for word in roboat_doc\n",
    "    if not word.is_stop and not word.is_punct\n",
    "]\n",
    "\n",
    "nouns=[]\n",
    "verbs=[]\n",
    "\n",
    "for word in roboat_doc:\n",
    "    print(word,\"===========\",word.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eb2cec7-947f-40fc-a0f4-f944d5bbc9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Nouns in text :  [country, emergency, approval, Pfizer, BioNTech, vaccine, firm, government, opportunities, vaccine, challenge, storage, degrees, experts, spokeswoman, dialogue, government, governments, world, infrastructure, requirements, country, plans, place, rollout]\n",
      "\n",
      "List of verbs in the text :  [grant, said, engaging, explore, roll, requires, told, committed, advance, working, understand, managed]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "with open('/Users/apple/Downloads/covid.txt') as file:\n",
    "    robot_text=file.read()\n",
    "\n",
    "roboat_doc=nlp(robot_text)\n",
    "\n",
    "roboat_doc = [\n",
    "    word\n",
    "    for word in roboat_doc\n",
    "    if not word.is_stop and not word.is_punct\n",
    "]\n",
    "\n",
    "nouns=[]\n",
    "verbs=[]\n",
    "\n",
    "for word in roboat_doc:\n",
    "    if word.pos_ == 'NOUN':\n",
    "        nouns.append(word)\n",
    "    if word.pos_ == 'VERB':\n",
    "        verbs.append(word)\n",
    "        \n",
    "print('List of Nouns in text : ',nouns)\n",
    "print()\n",
    "print('List of verbs in the text : ',verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ba0b0f6-b08f-40d6-96e0-48b78f6bae38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"d6902cdaba5c4fdba58d93badcaa38c5-0\" class=\"displacy\" width=\"925\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Kishan</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">teaching</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Data</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">science</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d6902cdaba5c4fdba58d93badcaa38c5-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,2.0 400.0,2.0 400.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d6902cdaba5c4fdba58d93badcaa38c5-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d6902cdaba5c4fdba58d93badcaa38c5-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d6902cdaba5c4fdba58d93badcaa38c5-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d6902cdaba5c4fdba58d93badcaa38c5-0-2\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d6902cdaba5c4fdba58d93badcaa38c5-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,179.0 L587,167.0 603,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d6902cdaba5c4fdba58d93badcaa38c5-0-3\" stroke-width=\"2px\" d=\"M420,177.0 C420,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d6902cdaba5c4fdba58d93badcaa38c5-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,179.0 L758.0,167.0 742.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# We can use display function for display\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "text='Kishan is teaching Data science'\n",
    "\n",
    "doc=nlp(text)\n",
    "displacy.render(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218a63a9-2ec2-49db-9ba7-55ffc2b9417e",
   "metadata": {},
   "source": [
    "## 14 Named Entity Recognition\n",
    "\n",
    "### Named Entity Recognition \n",
    "- Suppose you have a collection of news articles text data\n",
    "- From these we can capture\n",
    "  - Companies / Organization names\n",
    "  - People names\n",
    "- By using NER topic , we can do this\n",
    "\n",
    "\n",
    "## 15  What is NER.?\n",
    "- NER is the technique of identifying named entites in the text corpus and assigning them pre-defined categories such as 'person names' , 'locations','organization',etc..\n",
    "- It is very useful method especially in the field of classification problems and search engine optimization\n",
    "\n",
    "## 16 NER by using nltk and spacy\n",
    "- NER can be implemented through both nltk and spacy\n",
    "\n",
    "## 17 NER with spacy\n",
    "- Spacy is highly flexible and advanced library.\n",
    "- Named entity recognition through spacy is easy\n",
    "\n",
    "## 18 The few common lables are:\n",
    "- 'ORG'  :  companines, organization, etc.\n",
    "- 'PERSON'  : names of people\n",
    "- 'GPE'  : countries, states , etc\n",
    "- 'PRODUCT'  : vehicles, food products and so on\n",
    "- 'Language'  : Names of different languages\n",
    "\n",
    "- There are many other labels too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41415e0f-daca-4112-a408-bfde4abc59a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "london === GPE\n",
      "Google === ORG\n",
      "Mark === PERSON\n",
      "English === LANGUAGE\n"
     ]
    }
   ],
   "source": [
    "# Print all the named entities \n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sentence='The building is located at london. It is the headquarters of Google. Mark words there. He speaks English'\n",
    "\n",
    "doc=nlp(sentence)\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(entity.text,'===',entity.label_)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d242863a-ff48-46cd-ab14-9b31211c5e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The === <class 'spacy.tokens.token.Token'>\n",
      "building === <class 'spacy.tokens.token.Token'>\n",
      "is === <class 'spacy.tokens.token.Token'>\n",
      "located === <class 'spacy.tokens.token.Token'>\n",
      "at === <class 'spacy.tokens.token.Token'>\n",
      "london === <class 'spacy.tokens.token.Token'>\n",
      ". === <class 'spacy.tokens.token.Token'>\n",
      "It === <class 'spacy.tokens.token.Token'>\n",
      "is === <class 'spacy.tokens.token.Token'>\n",
      "the === <class 'spacy.tokens.token.Token'>\n",
      "headquarters === <class 'spacy.tokens.token.Token'>\n",
      "of === <class 'spacy.tokens.token.Token'>\n",
      "Google === <class 'spacy.tokens.token.Token'>\n",
      ". === <class 'spacy.tokens.token.Token'>\n",
      "Mark === <class 'spacy.tokens.token.Token'>\n",
      "words === <class 'spacy.tokens.token.Token'>\n",
      "there === <class 'spacy.tokens.token.Token'>\n",
      ". === <class 'spacy.tokens.token.Token'>\n",
      "He === <class 'spacy.tokens.token.Token'>\n",
      "speaks === <class 'spacy.tokens.token.Token'>\n",
      "English === <class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sentence='The building is located at london. It is the headquarters of Google. Mark words there. He speaks English'\n",
    "\n",
    "doc=nlp(sentence)\n",
    "\n",
    "for entity in doc:\n",
    "    print(entity,'===',type(entity))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73298575-eaea-4ff5-ac5f-7fbaae519c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "london === <class 'spacy.tokens.span.Span'>\n",
      "Google === <class 'spacy.tokens.span.Span'>\n",
      "Mark === <class 'spacy.tokens.span.Span'>\n",
      "English === <class 'spacy.tokens.span.Span'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sentence='The building is located at london. It is the headquarters of Google. Mark words there. He speaks English'\n",
    "\n",
    "doc=nlp(sentence)\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(entity,'===',type(entity))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bfaf468-143b-405e-a92c-ee61b46f042e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The building is located at \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    london\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ". It is the headquarters of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mark\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " words there. He speaks \n",
       "<mark class=\"entity\" style=\"background: #ff8197; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    English\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LANGUAGE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# colour in the text\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sentence='The building is located at london. It is the headquarters of Google. Mark words there. He speaks English'\n",
    "\n",
    "doc=nlp(sentence)\n",
    "\n",
    "displacy.render(doc,style='ent',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1b9b24-d346-48bb-b95a-9ad0e0decaaa",
   "metadata": {},
   "source": [
    "## Use case-1\n",
    "- Let us take the text data of collection of news headlines.\n",
    "- Can we get list of all names that have occurred in the news.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f46d4407-f86f-4742-806d-bae18a6e4a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Naredra', 'Modi', 'Twitter', 'Sonia', 'Gandhi', 'Rahul', 'Gandhi', 'Shah', 'Naredra', 'Modi', 'Rajnath', 'Singh']\n"
     ]
    }
   ],
   "source": [
    "# Display only person names from the text\n",
    "\n",
    "from spacy import displacy\n",
    "import spacy\n",
    "\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "news_articles = 'Honoured to serve India, Naredra Modi, 68, wrote in a Twitter post that popped up on the micro blogging site as the ceremony started at 7 p.m. before an audience of 8,000 people, who included United Progressive Alliance chairperson Sonia Gandhi and her son and Congress president Rahul Gandhi, both seated on the front row. Shah, 54, whose inclusion in the cabinet had been much speculated upon, was administered the oaths after Naredra Modi and Rajnath Singh, indicating the order of seniority in the new government, which took office exactly a week after results from the 17th general elections were declared'\n",
    "\n",
    "news_doc=nlp(news_articles)\n",
    "\n",
    "list_of_people=[]\n",
    "\n",
    "for token in news_doc:\n",
    "    if token.ent_type_=='PERSON':\n",
    "        list_of_people.append(token.text)\n",
    "\n",
    "print(list_of_people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250bc8f3-7086-4917-be95-89337f75052b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TensorFlow M1)",
   "language": "python",
   "name": "tf_m1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
