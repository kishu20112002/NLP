{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0539e123-6b54-4aaf-9090-cbcfab841646",
   "metadata": {},
   "source": [
    "# => Replacing and correcting words\n",
    "\n",
    "## 1 Text conversiond\n",
    "- We can convert the text from lower to upper and upper to lower case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cb0df45-8d98-42d7-bb0c-a0035f5d4df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO GOOD MORNING\n"
     ]
    }
   ],
   "source": [
    "# Converting lower case to upper case\n",
    "\n",
    "text=\"hello good morning\"\n",
    "print(text.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df5269d6-086b-4ce6-b0c6-c1202af84c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello good morning\n"
     ]
    }
   ],
   "source": [
    "# Converting upper case to lower case \n",
    "\n",
    "text=\"hello good morning\"\n",
    "print(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c522986c-10ed-4cfc-86ff-b9626bc0f80c",
   "metadata": {},
   "source": [
    "## 2 Removing numbers\n",
    "- By using regular expression we can remove numbers from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682e5dbb-5338-49da-870f-4e6753661a44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c069694-ee2f-46c4-bda3-0dced4be70ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement re (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[31mERROR: No matching distribution found for re\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98ec0453-b8c8-47e4-89de-1ae67e902079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Science  is better than  version\n"
     ]
    }
   ],
   "source": [
    "#  Removing numbers from the text\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "text = \"Data Science 2025 is better than 2020 version\"\n",
    "\n",
    "# remove numbers\n",
    "clean_text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "\n",
    "print(clean_text) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb1af4a-b4cb-4145-ae8a-613d586412a4",
   "metadata": {},
   "source": [
    "## 3 Removing punctuations\n",
    "- By using regular expression we can remove the punctuation from text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53fa4221-be5c-4902-920e-bf960411497b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is : Hello $@#$#Good !@#@morning#*#@&@#\n",
      "\n",
      "After punctuatiob : Hello Good morning\n"
     ]
    }
   ],
   "source": [
    "# Removing the punctuation from the text\n",
    "\n",
    "import re\n",
    "\n",
    "text=\"Hello $@#$#Good !@#@morning#*#@&@#\"\n",
    "\n",
    "print(\"Text is :\",text)\n",
    "\n",
    "res=re.sub(r'[^\\w\\s]','',text)\n",
    "\n",
    "print()\n",
    "print(\"After punctuatiob :\",res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23e7efb-7423-43dc-8071-a720322aedf5",
   "metadata": {},
   "source": [
    "## 4 Removing whitespaces\n",
    "- We can remove the whitespaces in string by using strip() method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8f0fc7f-6343-4add-8a74-a375707da609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  a sample string  \n",
      "\n",
      "a sample string\n"
     ]
    }
   ],
   "source": [
    "# Removing whitespaces from text \n",
    "\n",
    "text=\"  a sample string  \"\n",
    "\n",
    "print(text)\n",
    "print()\n",
    "res=text.strip()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31402884-d3bd-4ab3-9e02-b9bef55e9f6e",
   "metadata": {},
   "source": [
    "## 5 Part of Speech Tagging (POS)\n",
    "- The goal of POS is to assign the various parts of a speech to every word of the provided text like nouns, adjectives , verbs , etc\n",
    "- This is normally done based on the difinition and the context\n",
    "- Install textbloob library\n",
    "  - pip install textbloob\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e60d8a5-05c3-47a0-b8ee-4f44caad3134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement textbloob (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for textbloob\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install textbloob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d288dd92-1a02-4277-aff0-6f8f7f47e7f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Removing whitespaces from text \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtextblob\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "# Removing whitespaces from text \n",
    "\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc90d72e-fdca-481d-9f36-d8f536ac0678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: TextBlob in /opt/miniconda3/lib/python3.13/site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in /opt/miniconda3/lib/python3.13/site-packages (from TextBlob) (3.9.2)\n",
      "Requirement already satisfied: click in /opt/miniconda3/lib/python3.13/site-packages (from nltk>=3.9->TextBlob) (8.2.1)\n",
      "Requirement already satisfied: joblib in /opt/miniconda3/lib/python3.13/site-packages (from nltk>=3.9->TextBlob) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/miniconda3/lib/python3.13/site-packages (from nltk>=3.9->TextBlob) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/lib/python3.13/site-packages (from nltk>=3.9->TextBlob) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d75a6fb-efdb-4171-b0da-9920eda053f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Removing whitespaces from text \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtextblob\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      6\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maveraged_preceptron_tagger\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "# Removing whitespaces from text \n",
    "\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "\n",
    "nltk.download('averaged_preceptron_tagger')\n",
    "\n",
    "mystring=\"parts of speech : an article , to run, fascination, quickly and , of\"\n",
    "\n",
    "output=TextBlob(mystring)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bd89b63-5b0c-48c8-a15d-5dd44312da35",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtextblob\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      4\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maveraged_perceptron_tagger\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "mystring = \"parts of speech : an article , to run, fascination, quickly and , of\"\n",
    "\n",
    "output = TextBlob(mystring)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1824de5a-bcc0-46d3-b2e0-011084c316ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /opt/miniconda3/lib/python3.13/site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in /opt/miniconda3/lib/python3.13/site-packages (from textblob) (3.9.2)\n",
      "Requirement already satisfied: click in /opt/miniconda3/lib/python3.13/site-packages (from nltk>=3.9->textblob) (8.2.1)\n",
      "Requirement already satisfied: joblib in /opt/miniconda3/lib/python3.13/site-packages (from nltk>=3.9->textblob) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/miniconda3/lib/python3.13/site-packages (from nltk>=3.9->textblob) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/lib/python3.13/site-packages (from nltk>=3.9->textblob) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0222351-3259-4f38-ba35-e4c5c2f89561",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtextblob\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      4\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maveraged_perceptron_tagger\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "mystring = \"parts of speech : an article , to run, fascination, quickly and , of\"\n",
    "\n",
    "output = TextBlob(mystring)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "567b098c-1d53-43d0-8c4a-da750c26be7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Using cached textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting nltk>=3.9 (from textblob)\n",
      "  Using cached nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting click (from nltk>=3.9->textblob)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: joblib in /opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages (from nltk>=3.9->textblob) (1.5.3)\n",
      "Collecting regex>=2021.8.3 (from nltk>=3.9->textblob)\n",
      "  Downloading regex-2025.11.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk>=3.9->textblob)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "Using cached nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "Downloading regex-2025.11.3-cp310-cp310-macosx_11_0_arm64.whl (288 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk, textblob\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [textblob]3/5\u001b[0m [nltk]\n",
      "\u001b[1A\u001b[2KSuccessfully installed click-8.3.1 nltk-3.9.2 regex-2025.11.3 textblob-0.19.0 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "520315e0-1d38-419b-a318-77091f9eaac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74321b45-bafc-4be1-883b-bc06264e1a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/tf_m1/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83f999d2-21c6-43b6-8199-cdedd5f930ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/apple/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parts of speech : an article , to run, fascination, quickly and , of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "mystring = \"parts of speech : an article , to run, fascination, quickly and , of\"\n",
    "\n",
    "output = TextBlob(mystring)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "415bb3a0-bbc1-4053-af6d-b911bb16fa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parts of speech : an article , to run, fascination, quickly and , of\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "clean_text = re.sub(r'\\s+', ' ', mystring).strip()\n",
    "print(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c3aeb46-d15a-49b7-925b-2eb9fc38c5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/apple/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "MissingCorpusError",
     "evalue": "\nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages/textblob/decorators.py:35\u001b[0m, in \u001b[0;36mrequires_nltk_corpus.<locals>.decorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages/textblob/en/taggers.py:35\u001b[0m, in \u001b[0;36mNLTKTagger.tag\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     33\u001b[0m     text \u001b[38;5;241m=\u001b[39m tb\u001b[38;5;241m.\u001b[39mTextBlob(text)\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages/nltk/tag/__init__.py:168\u001b[0m, in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03mUse NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03mtag the given list of tokens.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m:rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages/nltk/tag/__init__.py:110\u001b[0m, in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages/nltk/tag/perceptron.py:180\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[0;34m(self, load, lang, loc)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages/nltk/tag/perceptron.py:277\u001b[0m, in \u001b[0;36mPerceptronTagger.load_from_json\u001b[0;34m(self, lang, loc)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m loc:\n\u001b[0;32m--> 277\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_param\u001b[39m(json_file):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng\u001b[0m\n\n  Searched in:\n    - '/Users/apple/nltk_data'\n    - '/opt/miniconda3/envs/tf_m1/nltk_data'\n    - '/opt/miniconda3/envs/tf_m1/share/nltk_data'\n    - '/opt/miniconda3/envs/tf_m1/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMissingCorpusError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m mystring \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts of speech : an article , to run, fascination, quickly and , of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m output \u001b[38;5;241m=\u001b[39m TextBlob(mystring)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtags\u001b[49m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages/textblob/decorators.py:23\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m---> 23\u001b[0m value \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages/textblob/blob.py:503\u001b[0m, in \u001b[0;36mBaseBlob.pos_tags\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns an list of tuples of the form (word, POS tag).\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \n\u001b[1;32m    486\u001b[0m \u001b[38;5;124;03mExample:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;124;03m:rtype: list of tuples\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, TextBlob):\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    502\u001b[0m         val\n\u001b[0;32m--> 503\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m [s\u001b[38;5;241m.\u001b[39mpos_tags \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentences]\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m sublist\n\u001b[1;32m    505\u001b[0m     ]\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    508\u001b[0m         (Word(\u001b[38;5;28mstr\u001b[39m(word), pos_tag\u001b[38;5;241m=\u001b[39mt), \u001b[38;5;28mstr\u001b[39m(t))\n\u001b[1;32m    509\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m word, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_tagger\u001b[38;5;241m.\u001b[39mtag(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m PUNCTUATION_REGEX\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;28mstr\u001b[39m(t))\n\u001b[1;32m    511\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages/textblob/blob.py:503\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns an list of tuples of the form (word, POS tag).\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \n\u001b[1;32m    486\u001b[0m \u001b[38;5;124;03mExample:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;124;03m:rtype: list of tuples\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, TextBlob):\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    502\u001b[0m         val\n\u001b[0;32m--> 503\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m [\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tags\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentences]\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m sublist\n\u001b[1;32m    505\u001b[0m     ]\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    508\u001b[0m         (Word(\u001b[38;5;28mstr\u001b[39m(word), pos_tag\u001b[38;5;241m=\u001b[39mt), \u001b[38;5;28mstr\u001b[39m(t))\n\u001b[1;32m    509\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m word, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_tagger\u001b[38;5;241m.\u001b[39mtag(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m PUNCTUATION_REGEX\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;28mstr\u001b[39m(t))\n\u001b[1;32m    511\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages/textblob/decorators.py:23\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m---> 23\u001b[0m value \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages/textblob/blob.py:509\u001b[0m, in \u001b[0;36mBaseBlob.pos_tags\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    502\u001b[0m         val\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m [s\u001b[38;5;241m.\u001b[39mpos_tags \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentences]\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m sublist\n\u001b[1;32m    505\u001b[0m     ]\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    508\u001b[0m         (Word(\u001b[38;5;28mstr\u001b[39m(word), pos_tag\u001b[38;5;241m=\u001b[39mt), \u001b[38;5;28mstr\u001b[39m(t))\n\u001b[0;32m--> 509\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m word, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tagger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m PUNCTUATION_REGEX\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;28mstr\u001b[39m(t))\n\u001b[1;32m    511\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf_m1/lib/python3.10/site-packages/textblob/decorators.py:37\u001b[0m, in \u001b[0;36mrequires_nltk_corpus.<locals>.decorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingCorpusError() \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merror\u001b[39;00m\n",
      "\u001b[0;31mMissingCorpusError\u001b[0m: \nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n"
     ]
    }
   ],
   "source": [
    "# Removing whitespaces from text\n",
    "\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "mystring = \"parts of speech : an article , to run, fascination, quickly and , of\"\n",
    "\n",
    "output = TextBlob(mystring)\n",
    "print(output.tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4030c659-8dd8-47bb-ab91-fa6f3412a861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/apple/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca06d569-a283-46a2-a3dc-05ebed09f731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/apple/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/apple/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package conll2000 to /Users/apple/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/apple/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "!python -m textblob.download_corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "298b2a0d-9369-4d1c-8b10-86f0a3950f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/apple/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ade76f44-fc2a-4307-9b2c-e0e83f2ce4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('an', 'DT'), ('article', 'NN'), ('to', 'TO'), ('run', 'VB'), ('fascination', 'NN'), ('quickly', 'RB'), ('and', 'CC'), ('of', 'IN')]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "\n",
    "mystring = \"parts of speech : an article , to run, fascination, quickly and , of\"\n",
    "\n",
    "output = TextBlob(mystring)\n",
    "print(output.tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdaabac4-51d1-44d9-8731-7b39de6c08ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('parts', 'NNS')\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "\n",
    "mystring = \"parts of speech : an article , to run, fascination, quickly and , of\"\n",
    "\n",
    "output = TextBlob(mystring)\n",
    "print(output.tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d865edce-2e88-4536-9b6f-e7d94d2a1b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parts of speech : an article , to run, fascination, quickly and , of\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "\n",
    "mystring = \"parts of speech : an article , to run, fascination, quickly and , of\"\n",
    "\n",
    "output = TextBlob(mystring)\n",
    "\n",
    "print(mystring)\n",
    "\n",
    "\n",
    "for value in output.tags:\n",
    "    if output.tags[0][1]=='VBZ':\n",
    "        print(value)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2765d414-1c9c-4381-a1e6-b0545e54895c",
   "metadata": {},
   "source": [
    "# Common Part of Speech Tagging POS tages \n",
    "\n",
    "## Nouns\n",
    "\n",
    "| Tag      | Meaning                | Example       |\n",
    "| -------- | ---------------------- | ------------- |\n",
    "| **NN**   | Noun (singular)        | book, car     |\n",
    "| **NNS**  | Noun (plural)          | books         |\n",
    "| **NNP**  | Proper noun (singular) | India, Daniel |\n",
    "| **NNPS** | Proper noun (plural)   | Indians       |\n",
    "\n",
    "\n",
    "## Verbs\n",
    "\n",
    "| Tag     | Meaning                             | Example |\n",
    "| ------- | ----------------------------------- | ------- |\n",
    "| **VB**  | Verb (base form)                    | run     |\n",
    "| **VBD** | Verb (past tense)                   | ran     |\n",
    "| **VBG** | Verb (gerund/-ing)                  | running |\n",
    "| **VBN** | Verb (past participle)              | eaten   |\n",
    "| **VBP** | Verb (present, non-3rd person)      | run     |\n",
    "| **VBZ** | Verb (present, 3rd person singular) | runs    |\n",
    "\n",
    "\n",
    "\n",
    "## Adjectives\n",
    "\n",
    "| Tag     | Meaning               | Example |\n",
    "| ------- | --------------------- | ------- |\n",
    "| **JJ**  | Adjective             | good    |\n",
    "| **JJR** | Comparative adjective | better  |\n",
    "| **JJS** | Superlative adjective | best    |\n",
    "\n",
    "\n",
    "## Adverbs\n",
    "\n",
    "| Tag     | Meaning            | Example |\n",
    "| ------- | ------------------ | ------- |\n",
    "| **RB**  | Adverb             | quickly |\n",
    "| **RBR** | Comparative adverb | faster  |\n",
    "| **RBS** | Superlative adverb | fastest |\n",
    "\n",
    "\n",
    "\n",
    "## Other Imprtant tages\n",
    "\n",
    "\n",
    "| Tag      | Meaning                   | Example    |\n",
    "| -------- | ------------------------- | ---------- |\n",
    "| **IN**   | Preposition / conjunction | in, of     |\n",
    "| **DT**   | Determiner                | a, an, the |\n",
    "| **PRP**  | Personal pronoun          | I, you     |\n",
    "| **PRP$** | Possessive pronoun        | my, his    |\n",
    "| **CC**   | Coordinating conjunction  | and, but   |\n",
    "| **TO**   | to                        | to         |\n",
    "| **CD**   | Cardinal number           | one, 2     |\n",
    "\n",
    "\n",
    "\n",
    "### => Why POS tags matter in NLP\n",
    "\n",
    "- Lemmatization\n",
    "\n",
    "- Named Entity Recognition\n",
    "\n",
    "- Syntax analysis\n",
    "\n",
    "- Chatbots\n",
    "\n",
    "- Search engines\n",
    "\n",
    "\n",
    "### => One-Line Exam Answer (MLA)\n",
    "POS tags represent the grammatical category of words such as noun, verb, adjective, etc., commonly using the Penn Treebank tagset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68f2df19-729c-454a-857d-5d0035e5f158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run VB\n"
     ]
    }
   ],
   "source": [
    "for word, tag in output.tags:\n",
    "    if tag.startswith('VB'):\n",
    "        print(word, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7cc32af-be3d-40ff-bab5-f3907241f9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syn tag :  n\n",
      "syn tag :  v\n",
      "syn tag :  a\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "syn=wordnet.synsets('hello')[0]\n",
    "print(\"syn tag : \",syn.pos())\n",
    "\n",
    "syn=wordnet.synsets('doing')[0]\n",
    "print(\"syn tag : \", syn.pos())\n",
    "\n",
    "syn=wordnet.synsets(\"beautiful\")[0]\n",
    "print(\"syn tag : \", syn.pos())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b50873c-dd03-4dba-bd9d-4200d8cb612e",
   "metadata": {},
   "source": [
    "## 6 Information Extraction \n",
    "- We need to understand the tags and parsers to build infomation extraction engine\n",
    "- Let us see a basic infomation extraction pipline\n",
    "\n",
    "\n",
    "| **Step** | **Component**          | **Input**                | **Output**            | **Purpose**                                   |\n",
    "| -------- | ---------------------- | ------------------------ | --------------------- | --------------------------------------------- |\n",
    "| 1        | Raw Text               | Text document / sentence | Text                  | Original unstructured data                    |\n",
    "| 2        | Sentence Tokenization  | Raw text                 | List of sentences     | Split text into sentences                     |\n",
    "| 3        | Word Tokenization      | Sentences                | List of words         | Split sentences into words                    |\n",
    "| 4        | POS Tagging            | Tokens                   | (Word, POS tag) pairs | Identify grammatical roles (NN, VB, JJ, etc.) |\n",
    "| 5        | Entity Detection (NER) | POS-tagged text          | Named entities        | Identify names, places, organizations, dates  |\n",
    "| 6        | Relation Extraction    | Entities + context       | Entity relations      | Find relationships between entities           |\n",
    "| 7        | Structured Output      | Relations                | Tables / JSON | Graph | Convert text into structured data             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8597f712-6ef3-4b51-8f0e-efafe7794d46",
   "metadata": {},
   "source": [
    "## 7 Infomation extraction has many application including \n",
    "- Business intelligence\n",
    "- Resumne harvesting\n",
    "- Media analysis\n",
    "- Sentiment detection\n",
    "- Partent search\n",
    "- Email scanning\n",
    "\n",
    "## 8 Collocation : Bigrams and Trigrams\n",
    "\n",
    "### What is collocation ?\n",
    "- Collocations are the pairs of words occurring together many times in paragraphs\n",
    "- It is calculated by the number of those pair occuring together to the overall words count of the paragraph\n",
    "- We can say that finding collocations requires calculation the frequecies of words and their appearance in the context of other words\n",
    "\n",
    "### Bigrams and Trigrams\n",
    "- Collocation can be categorized into two types\n",
    " - Bigrams combination of two words\n",
    " - Trigrams combination of three words\n",
    "- Bigrams and Trigram provide more meaningd and useful feature for the feature extraction stage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "186b8c11-47dc-4fbe-b34e-03f279f51e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Data', 'science'), ('science', 'is'), ('is', 'a'), ('a', 'totally'), ('totally', 'new'), ('new', 'kind'), ('kind', 'of'), ('of', 'learning'), ('learning', 'experience')]\n"
     ]
    }
   ],
   "source": [
    "# Bigram example  : Bigrams combination of two words\n",
    "\n",
    "import nltk \n",
    "\n",
    "text=\"Data science is a totally new kind of learning experience\"\n",
    "Tokens=nltk.word_tokenize(text)\n",
    "\n",
    "output=list(nltk.bigrams(Tokens))\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb6886bc-da9f-4d8f-8ce8-b11c2f7cdd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Data', 'science', 'is'), ('science', 'is', 'a'), ('is', 'a', 'totally'), ('a', 'totally', 'new'), ('totally', 'new', 'kind'), ('new', 'kind', 'of'), ('kind', 'of', 'learning'), ('of', 'learning', 'experience')]\n"
     ]
    }
   ],
   "source": [
    "# Trigram example    : Trigrams combination of three words\n",
    "\n",
    "import nltk \n",
    "\n",
    "text=\"Data science is a totally new kind of learning experience\"\n",
    "Tokens=nltk.word_tokenize(text)\n",
    "\n",
    "output=list(nltk.trigrams(Tokens))\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2719118-2404-499d-856b-0c93c535083c",
   "metadata": {},
   "source": [
    "\n",
    "## 9 Wordnet  ### interviw question\n",
    "\n",
    "- Wordnet is an NLTK lexical database for English\n",
    "- It can be used to find the meaning of words, synonym or antonym.\n",
    "\n",
    "### synset\n",
    "- Synset is a special a kind of a simple interface that is present in NLTK to look up words in Wordnet\n",
    "- Synset instances are the groupings of synoymous words that express the same concept "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfca289-1371-45dc-81e4-38df2894e9ac",
   "metadata": {},
   "source": [
    "# NLP  => nltk => Initial version \n",
    "# spacy => adv version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81631c67-0646-4b2b-abaa-5186f96a95f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symset name : hello.n.01\n",
      "symset meaning :  an expression of greeting\n",
      "synset example :  ['every morning they exchanged polite hellos']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "word=\"hello\"\n",
    "syn=wordnet.synsets(word)[0]\n",
    "\n",
    "print(\"symset name :\",syn.name())\n",
    "print(\"symset meaning : \",syn.definition())\n",
    "print(\"synset example : \", syn.examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68fb49ad-e0c1-4ac7-9521-fa6db579d401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symset name : male_child.n.01\n",
      "symset meaning :  a youthful male person\n",
      "synset example :  ['the baby was a boy', 'she made the boy brush his teeth every night', 'most soldiers are only boys in uniform']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "word=\"boy\"\n",
    "syn=wordnet.synsets(word)[0]\n",
    "\n",
    "print(\"symset name :\",syn.name())\n",
    "print(\"symset meaning : \",syn.definition())\n",
    "print(\"synset example : \", syn.examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37566b74-bf7d-4b2c-b5b8-eefb8142d26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symset name : girl.n.01\n",
      "symset meaning :  a young woman\n",
      "synset example :  ['a young lady of 18']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "word=\"girl\"\n",
    "syn=wordnet.synsets(word)[0]\n",
    "\n",
    "print(\"symset name :\",syn.name())\n",
    "print(\"symset meaning : \",syn.definition())\n",
    "print(\"synset example : \", syn.examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c340b401-98dc-43b8-8d1c-7fb87fe1a7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TensorFlow M1)",
   "language": "python",
   "name": "tf_m1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
